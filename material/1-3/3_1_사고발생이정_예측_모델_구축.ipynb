{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# 1. 사고발생이정 예측 모델 구축\n\n## 1.1. 실습 개요\n\n이 노트북에서는 고속도로 교통사고 데이터를 기반으로 **사고발생이정(사고 발생 지점의 기점 거리, km)** 을 예측하는 머신러닝 회귀 모델을 구축합니다.\n\n## 1.2. 사고심각도 vs 사고발생이정 비교\n\n| 항목 | 사고심각도 | 사고발생이정 |\n|------|-----------|-------------|\n| 정의 | 사망 × 3 + 부상 | 기점으로부터의 거리 (km) |\n| 예측 가능성 | 낮음 (불규칙적) | 높음 (구조적 관계 존재) |\n| 핵심 설명 변수 | 사망, 부상 (제외됨) | 노선명, 방향 |\n| 예상 성능 | R² < 0 | R² > 0 (개선 기대) |\n\n## 1.3. 학습 목표\n\n1. 예측 가능한 타깃 변수의 특성 이해\n2. 노선/방향 정보가 위치 예측에 미치는 영향 분석\n3. Feature Importance를 통한 주요 변수 탐색\n4. 사고심각도 예측 결과와 비교하여 타깃 선택의 중요성 체험\n\n## 1.4. 사용 모델\n\n이전 노트북과 동일한 4가지 회귀 모델을 사용합니다:\n- Linear Regression (Baseline)\n- RandomForest Regressor\n- XGBoost Regressor\n- LightGBM Regressor",
   "metadata": {
    "id": "eY8880rdThJX"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 515
    },
    "id": "jXqwu9wkR-mt",
    "outputId": "6b9da3b9-0468-4b76-aa0c-a715fb8535d3"
   },
   "outputs": [],
   "source": "# ============================================\n# 2. 패키지 설치 및 한글 폰트 설정\n# ============================================\n\n# --------------------------------------------\n# 2.1. 추가 패키지 설치\n# --------------------------------------------\n!pip install xgboost lightgbm -q\n\n# --------------------------------------------\n# 2.2. 한글 폰트 설정 (코랩 전용)\n# --------------------------------------------\n!apt-get update -qq\n!apt-get install -y fonts-nanum\n\nimport matplotlib as mpl\nimport shutil\n\nroot = mpl.matplotlib_fname().replace(\"matplotlibrc\", \"\")\ntarget_font = root + \"fonts/ttf/DejaVuSans.ttf\"\nnanum_font = \"/usr/share/fonts/truetype/nanum/NanumGothic.ttf\"\nshutil.copyfile(nanum_font, target_font)\n\n!rm -rf ~/.cache/matplotlib\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\n\n# 한글 출력 테스트\nplt.figure(figsize=(4, 3))\nplt.title(\"한글 폰트 정상 출력 확인\")\nplt.xlabel(\"가로축\")\nplt.ylabel(\"세로축\")\nplt.plot([1, 2, 3], [1, 4, 2])\nplt.show()"
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 3. 라이브러리 임포트 및 데이터 로드\n# ============================================\n\nimport numpy as np\nimport pandas as pd\nfrom math import sqrt\n\n# scikit-learn 관련 모듈\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\n# 회귀 모델들\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\n# 데이터 로드\nfile_path = \"highway_accident_fe.csv\"\ndf = pd.read_csv(file_path)\n\nprint(\"데이터 크기:\", df.shape)\ndisplay(df.head())",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "id": "D7jbCV4KSV8O",
    "outputId": "dbb6c0fe-0f8d-425e-ed35-3eae80ab9467"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 4. 타깃/특징 분리 및 피처 타입 분류\n# ============================================\n\n# --------------------------------------------\n# 4.1. 타깃 변수 정의\n# --------------------------------------------\n# - 이번에는 사고발생이정을 예측 (사고심각도와 다른 타깃)\ntarget_col = \"사고발생이정\"\n\n# --------------------------------------------\n# 4.2. 제외할 컬럼 정의\n# --------------------------------------------\n# - 사고발생이정: 예측 타깃이므로 X에서 제외\n# - 사고심각도: 이번 타깃과 무관한 변수\n# - 사고일자: 고유값이 너무 많음\ndrop_cols = [\"사고발생이정\", \"사고심각도\", \"사고일자\"]\ndrop_cols = [c for c in drop_cols if c in df.columns]\n\n# X: 입력 특징, y: 타깃\nX = df.drop(columns=drop_cols)\ny = df[target_col]\n\nprint(\"입력 특징 컬럼:\", X.columns.tolist())\n\n# --------------------------------------------\n# 4.3. 범주형 / 수치형 컬럼 자동 분류\n# --------------------------------------------\ncategorical_cols = X.select_dtypes(include=[\"object\"]).columns.tolist()\nnumeric_cols = X.select_dtypes(include=[\"int64\", \"float64\", \"int32\", \"float32\"]).columns.tolist()\n\nprint(\"\\n범주형 컬럼:\", categorical_cols)\nprint(\"수치형 컬럼:\", numeric_cols)",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZvY0uWmwSV6B",
    "outputId": "b7a443ca-a72b-4a37-fb50-70ad8ea79643"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 5. 전처리기(Preprocessor) 구성\n# ============================================\n\n# - 범주형: OneHotEncoder\n# - 수치형: passthrough (그대로 통과)\ncategorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"cat\", categorical_transformer, categorical_cols),\n        (\"num\", \"passthrough\", numeric_cols),\n    ]\n)",
   "metadata": {
    "id": "goB22wplSV3-"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 6. 학습/테스트 데이터 분리\n# ============================================\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\nprint(\"학습 데이터 크기:\", X_train.shape)\nprint(\"테스트 데이터 크기:\", X_test.shape)",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fU3wEdmISV1y",
    "outputId": "401b45bc-8371-476a-945a-8c32fd7c824b"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 7. 회귀 모델 정의\n# ============================================\n\nmodels = {\n    \"RandomForest\": RandomForestRegressor(\n        n_estimators=300,\n        random_state=42,\n        n_jobs=-1,\n    ),\n    \"XGBoost\": XGBRegressor(\n        n_estimators=300,\n        max_depth=6,\n        learning_rate=0.1,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        objective=\"reg:squarederror\",\n        random_state=42,\n        n_jobs=-1\n    ),\n    \"LightGBM\": LGBMRegressor(\n        n_estimators=300,\n        learning_rate=0.1,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42,\n        n_jobs=-1\n    ),\n    \"LinearRegression\": LinearRegression()\n}\n\n# 성능 저장용 리스트\nmetrics_list = []\n\n# Feature Importance 저장용 딕셔너리\nfeature_importances_dict = {}",
   "metadata": {
    "id": "fR_QuyEWShDE"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 8. 모델 학습 및 평가\n# ============================================\n\nfor model_name, model in models.items():\n    print(f\"\\n{'=' * 50}\")\n    print(f\"{model_name} 학습 중...\")\n    print(\"=\" * 50)\n    \n    # --------------------------------------------\n    # 8.1. Pipeline 구성 및 학습\n    # --------------------------------------------\n    pipe = Pipeline([\n        (\"preprocessor\", preprocessor),\n        (\"model\", model),\n    ])\n    \n    pipe.fit(X_train, y_train)\n    y_pred = pipe.predict(X_test)\n    \n    # --------------------------------------------\n    # 8.2. 평가 지표 계산\n    # --------------------------------------------\n    mae = mean_absolute_error(y_test, y_pred)\n    rmse = sqrt(mean_squared_error(y_test, y_pred))\n    r2 = r2_score(y_test, y_pred)\n    \n    metrics_list.append({\n        \"model\": model_name,\n        \"MAE\": mae,\n        \"RMSE\": rmse,\n        \"R2\": r2\n    })\n    \n    print(f\"MAE: {mae:.4f}\")\n    print(f\"RMSE: {rmse:.4f}\")\n    print(f\"R²: {r2:.4f}\")\n    \n    # --------------------------------------------\n    # 8.3. Feature Importance 추출\n    # --------------------------------------------\n    preproc = pipe.named_steps[\"preprocessor\"]\n    feature_names = preproc.get_feature_names_out()\n    fitted_model = pipe.named_steps[\"model\"]\n    \n    if hasattr(fitted_model, \"feature_importances_\"):\n        importances = fitted_model.feature_importances_\n    elif hasattr(fitted_model, \"coef_\"):\n        importances = np.abs(fitted_model.coef_)\n    else:\n        importances = None\n    \n    if importances is not None:\n        fi_df = pd.DataFrame({\n            \"feature\": feature_names,\n            \"importance\": importances\n        }).sort_values(\"importance\", ascending=False)\n        \n        feature_importances_dict[model_name] = fi_df\n        print(f\"\\n[{model_name}] 상위 10개 Feature Importance:\")\n        print(fi_df.head(10).to_string(index=False))",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PJv_EVZlShA2",
    "outputId": "385c1ce3-c799-486d-db7e-eb26c2366d91"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 9. 모델 성능 비교 및 시각화\n# ============================================\n\nmetrics_df = pd.DataFrame(metrics_list)\nprint(\"\\n\" + \"=\" * 50)\nprint(\"모델 성능 비교 (타깃: 사고발생이정)\")\nprint(\"=\" * 50)\ndisplay(metrics_df)\n\n# RMSE 기준 정렬\nmetrics_df_sorted = metrics_df.sort_values(\"RMSE\")\nprint(\"\\n[RMSE 기준 정렬]\")\ndisplay(metrics_df_sorted)\n\n# --------------------------------------------\n# 9.1. RMSE 막대그래프\n# --------------------------------------------\nplt.figure(figsize=(8, 4))\nsns.barplot(data=metrics_df_sorted, x=\"model\", y=\"RMSE\", palette=\"Blues_d\")\nplt.title(\"모델별 RMSE 비교 (타깃: 사고발생이정)\")\nplt.xlabel(\"모델\")\nplt.ylabel(\"RMSE\")\nplt.tight_layout()\nplt.show()\n\n# --------------------------------------------\n# 9.2. R² 막대그래프\n# --------------------------------------------\nplt.figure(figsize=(8, 4))\nsns.barplot(\n    data=metrics_df.sort_values(\"R2\", ascending=False),\n    x=\"model\", y=\"R2\", palette=\"Greens_d\"\n)\nplt.title(\"모델별 R² 비교 (타깃: 사고발생이정)\")\nplt.xlabel(\"모델\")\nplt.ylabel(\"R²\")\nplt.axhline(y=0, color=\"gray\", linestyle=\"--\", linewidth=1)\nplt.tight_layout()\nplt.show()",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "CJsd2XDMSg-g",
    "outputId": "4119d4d5-9d91-41a6-e308-23c15baae33a"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# 10. 결과 해석 및 실습 정리\n\n## 10.1. 핵심 인사이트: Feature Importance 분석\n\n모든 모델에서 공통적으로 **노선명**이 가장 중요한 변수로 나타났습니다.\n\n| 모델 | 상위 중요 피처 |\n|------|----------------|\n| RandomForest | 경부선, 사고분, 사고일, 서해안선, 사고시 |\n| XGBoost | 경부선, 방향_하남, 중앙선, 제2중부선 |\n| LightGBM | 사고분, 사고일, 사고시, 사고월 |\n| LinearRegression | 수도권제2순환선, 경부선, 제2중부선 |\n\n**해석**: 노선명은 사고발생이정(km)과 직접적인 관계가 있습니다.\n- 각 노선마다 고유한 길이와 특성이 있음\n- 경부선은 길이가 길어 사고발생이정 값의 범위가 넓음\n- 방향 정보도 위치 예측에 도움이 됨\n\n## 10.2. 사고심각도 vs 사고발생이정 비교\n\n| 항목 | 사고심각도 예측 | 사고발생이정 예측 |\n|------|----------------|------------------|\n| R² 결과 | 음수 (baseline보다 나쁨) | 양수 또는 개선됨 |\n| 핵심 변수 | 사망/부상 (제외됨) | 노선명/방향 (포함됨) |\n| 예측 난이도 | 매우 어려움 | 상대적으로 쉬움 |\n\n## 10.3. 3일간 실습 정리\n\n### Day 1: 데이터 수집 및 기초 분석\n- API를 활용한 공공데이터 수집\n- 기초 시각화 및 통계량 확인\n\n### Day 2: EDA 심화 및 모델링\n- Feature Engineering (파생변수 생성)\n- 다차원 EDA (노선별, 시간대별, 원인별 분석)\n- 사고심각도 예측 모델 구축 및 한계 확인\n\n### Day 3: 타깃 재설정 및 모델링\n- 사고발생이정 예측 모델 구축\n- 타깃 선택의 중요성 체험\n- Feature Importance를 통한 인사이트 도출\n\n## 10.4. 핵심 교훈\n\n1. **타깃 변수 선택이 중요**: 예측하려는 것과 관련된 정보가 데이터에 있어야 함\n2. **데이터 누설(Leakage) 주의**: 타깃과 직접 관련된 변수는 제외해야 함\n3. **도메인 지식 활용**: 노선명이 위치와 관련있다는 도메인 지식이 모델 해석에 도움\n4. **모델은 도구일 뿐**: 좋은 모델도 좋은 데이터 없이는 무용지물",
   "metadata": {
    "id": "maqEagw5SVhj"
   }
  }
 ]
}