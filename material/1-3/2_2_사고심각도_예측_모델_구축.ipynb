{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# 1. 사고심각도 예측 모델 구축\n\n## 1.1. 실습 개요\n\n이 노트북에서는 고속도로 교통사고 데이터를 기반으로 **사고심각도(사망 × 3 + 부상)** 를 예측하는 머신러닝 회귀 모델을 구축합니다.\n\n단순히 모델을 학습시키는 것을 넘어서, **모델의 성능을 올바르게 해석하고 데이터 기반 의사결정 능력을 기르는 것**이 핵심 목표입니다.\n\n## 1.2. 학습 목표\n\n1. scikit-learn Pipeline을 활용한 전처리 + 모델 통합\n2. ColumnTransformer와 OneHotEncoder를 활용한 범주형 변수 처리\n3. 4가지 회귀 모델 비교 (Linear, RandomForest, XGBoost, LightGBM)\n4. 평가 지표(MAE, RMSE, R²) 해석\n5. Feature Importance 분석\n\n## 1.3. 사용 모델\n\n| 모델 | 목적 | 특징 |\n|------|------|------|\n| Linear Regression | Baseline | 단순 선형 관계 확인 |\n| RandomForest | 안정적 예측 | 과적합에 강건, 해석 용이 |\n| XGBoost | 고성능 | 비선형 패턴 학습 우수 |\n| LightGBM | 고속·고성능 | 대규모 범주형 처리 강점 |\n\n## 1.4. 평가 지표\n\n| 지표 | 설명 | 해석 |\n|------|------|------|\n| MAE | 평균 절대 오차 | 낮을수록 좋음 |\n| RMSE | 평균 제곱근 오차 | 큰 오차에 민감, 낮을수록 좋음 |\n| R² | 결정 계수 | 1에 가까울수록 좋음, 음수면 baseline보다 나쁨 |\n\n## 1.5. 중요 참고사항\n\n이번 실습에서는 **사망·부상 컬럼을 모델 입력에서 제거**합니다.\n- 이유: 사고심각도 = 사망 × 3 + 부상 이므로, 사망·부상을 포함하면 정보 누설(leakage) 발생\n- 결과: 나머지 피처만으로는 예측이 어려워 성능이 낮게 나올 수 있음\n- 의의: \"타깃 관련 정보 누설을 제거하면 모델이 어떻게 영향받는지\" 학습",
   "metadata": {
    "id": "SMmIiQb1-wHj"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 515
    },
    "id": "edxQpu-s4slr",
    "outputId": "0fb06028-2dda-460c-df48-25880cabaae6"
   },
   "outputs": [],
   "source": "# ============================================\n# 2. 패키지 설치 및 한글 폰트 설정\n# ============================================\n\n# --------------------------------------------\n# 2.1. 추가 패키지 설치\n# --------------------------------------------\n# - XGBoost, LightGBM은 코랩에 기본 설치되어 있지 않을 수 있음\n!pip install xgboost lightgbm -q\n\n# --------------------------------------------\n# 2.2. 한글 폰트 설정 (코랩 전용)\n# --------------------------------------------\n!apt-get update -qq\n!apt-get install -y fonts-nanum\n\nimport matplotlib as mpl\nimport shutil\n\nroot = mpl.matplotlib_fname().replace(\"matplotlibrc\", \"\")\ntarget_font = root + \"fonts/ttf/DejaVuSans.ttf\"\nnanum_font = \"/usr/share/fonts/truetype/nanum/NanumGothic.ttf\"\nshutil.copyfile(nanum_font, target_font)\n\n!rm -rf ~/.cache/matplotlib\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\n\n# 한글 출력 테스트\nplt.figure(figsize=(4, 3))\nplt.title(\"한글 폰트 정상 출력 확인\")\nplt.xlabel(\"가로축\")\nplt.ylabel(\"세로축\")\nplt.plot([1, 2, 3], [1, 4, 2])\nplt.show()"
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 3. 라이브러리 임포트 및 데이터 로드\n# ============================================\n\nimport numpy as np\nimport pandas as pd\nfrom math import sqrt\n\n# scikit-learn 관련 모듈\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\n# 회귀 모델들\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\n\n# 데이터 로드\nfile_path = \"highway_accident_fe.csv\"\ndf = pd.read_csv(file_path)\n\nprint(\"데이터 크기:\", df.shape)\ndisplay(df.head())",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 223
    },
    "id": "OWzRcj5K6OhC",
    "outputId": "d6c99e1a-6cd5-447c-e9c0-921892c8a7a9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 4. 타깃/특징 분리 및 피처 타입 분류\n# ============================================\n\n# --------------------------------------------\n# 4.1. 타깃 변수 정의\n# --------------------------------------------\ntarget_col = \"사고심각도\"\n\n# --------------------------------------------\n# 4.2. 제외할 컬럼 정의\n# --------------------------------------------\n# - 사고심각도: 예측 타깃이므로 X에서 제외\n# - 사망, 부상: 사고심각도 계산에 사용된 정보 (leakage 방지)\n# - 사고일자: 고유값이 너무 많아 직접적 의미 없음\ndrop_cols = [\"사고심각도\", \"사망\", \"부상\", \"사고일자\"]\ndrop_cols = [c for c in drop_cols if c in df.columns]\n\n# X: 입력 특징, y: 타깃\nX = df.drop(columns=drop_cols)\ny = df[target_col]\n\nprint(\"입력 특징 컬럼:\", X.columns.tolist())\n\n# --------------------------------------------\n# 4.3. 범주형 / 수치형 컬럼 자동 분류\n# --------------------------------------------\n# - select_dtypes(): 데이터 타입에 따라 컬럼 선택\ncategorical_cols = X.select_dtypes(include=[\"object\"]).columns.tolist()\nnumeric_cols = X.select_dtypes(include=[\"int64\", \"float64\", \"int32\", \"float32\"]).columns.tolist()\n\nprint(\"\\n범주형 컬럼:\", categorical_cols)\nprint(\"수치형 컬럼:\", numeric_cols)",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AvkKzTa-6Vuf",
    "outputId": "e4494a5a-8455-4edf-8026-7bdc6c90f1bd"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 5. 전처리기(Preprocessor) 구성\n# ============================================\n\n# --------------------------------------------\n# 5.1. OneHotEncoder 설정\n# --------------------------------------------\n# - 범주형 변수를 0/1 더미 변수로 변환\n# - handle_unknown=\"ignore\": 학습 시 없던 새로운 값이 나와도 에러 없이 처리\ncategorical_transformer = OneHotEncoder(handle_unknown=\"ignore\")\n\n# --------------------------------------------\n# 5.2. ColumnTransformer 설정\n# --------------------------------------------\n# - 서로 다른 컬럼 그룹에 서로 다른 전처리기 적용\n# - cat: 범주형 컬럼 → OneHotEncoder\n# - num: 수치형 컬럼 → 그대로 통과 (passthrough)\npreprocessor = ColumnTransformer(\n    transformers=[\n        (\"cat\", categorical_transformer, categorical_cols),\n        (\"num\", \"passthrough\", numeric_cols),\n    ]\n)",
   "metadata": {
    "id": "8IeSg32g6bcC"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 6. 학습/테스트 데이터 분리\n# ============================================\n\n# - train_test_split(): 데이터를 학습용/테스트용으로 분리\n# - test_size=0.2: 20%는 테스트, 80%는 학습\n# - random_state=42: 재현성을 위한 난수 시드 고정\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\n\nprint(\"학습 데이터 크기:\", X_train.shape)\nprint(\"테스트 데이터 크기:\", X_test.shape)",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DSp-tD2Z6eGh",
    "outputId": "152e7c53-8eb7-46ff-c083-172da5d12b95"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 7. 회귀 모델 정의\n# ============================================\n\nmodels = {\n    \"RandomForest\": RandomForestRegressor(\n        n_estimators=300,      # 트리 개수\n        random_state=42,\n        n_jobs=-1,             # 모든 CPU 코어 사용\n    ),\n    \"XGBoost\": XGBRegressor(\n        n_estimators=300,\n        max_depth=6,\n        learning_rate=0.1,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        objective=\"reg:squarederror\",\n        random_state=42,\n        n_jobs=-1\n    ),\n    \"LightGBM\": LGBMRegressor(\n        n_estimators=300,\n        learning_rate=0.1,\n        subsample=0.8,\n        colsample_bytree=0.8,\n        random_state=42,\n        n_jobs=-1\n    ),\n    \"LinearRegression\": LinearRegression()\n}\n\n# 성능 저장용 리스트\nmetrics_list = []\n\n# Feature Importance 저장용 딕셔너리\nfeature_importances_dict = {}",
   "metadata": {
    "id": "oBGoi_g66kEB"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 8. 모델 학습 및 평가\n# ============================================\n\nfor model_name, model in models.items():\n    print(f\"\\n{'=' * 50}\")\n    print(f\"{model_name} 학습 중...\")\n    print(\"=\" * 50)\n    \n    # --------------------------------------------\n    # 8.1. Pipeline 구성\n    # --------------------------------------------\n    # - 전처리(preprocessor)와 모델(model)을 하나로 묶음\n    # - fit() 호출 시 전처리 → 모델 학습이 순차 수행\n    pipe = Pipeline([\n        (\"preprocessor\", preprocessor),\n        (\"model\", model),\n    ])\n    \n    # --------------------------------------------\n    # 8.2. 학습 및 예측\n    # --------------------------------------------\n    pipe.fit(X_train, y_train)\n    y_pred = pipe.predict(X_test)\n    \n    # --------------------------------------------\n    # 8.3. 평가 지표 계산\n    # --------------------------------------------\n    mae = mean_absolute_error(y_test, y_pred)\n    rmse = sqrt(mean_squared_error(y_test, y_pred))\n    r2 = r2_score(y_test, y_pred)\n    \n    metrics_list.append({\n        \"model\": model_name,\n        \"MAE\": mae,\n        \"RMSE\": rmse,\n        \"R2\": r2\n    })\n    \n    print(f\"MAE: {mae:.4f}\")\n    print(f\"RMSE: {rmse:.4f}\")\n    print(f\"R²: {r2:.4f}\")\n    \n    # --------------------------------------------\n    # 8.4. Feature Importance 추출\n    # --------------------------------------------\n    # - get_feature_names_out(): 전처리 후 생성된 피처 이름 목록\n    preproc = pipe.named_steps[\"preprocessor\"]\n    feature_names = preproc.get_feature_names_out()\n    fitted_model = pipe.named_steps[\"model\"]\n    \n    # 트리 모델: feature_importances_ 속성 사용\n    # 선형 모델: coef_ 속성의 절대값 사용\n    if hasattr(fitted_model, \"feature_importances_\"):\n        importances = fitted_model.feature_importances_\n    elif hasattr(fitted_model, \"coef_\"):\n        importances = np.abs(fitted_model.coef_)\n    else:\n        importances = None\n    \n    if importances is not None:\n        fi_df = pd.DataFrame({\n            \"feature\": feature_names,\n            \"importance\": importances\n        }).sort_values(\"importance\", ascending=False)\n        \n        feature_importances_dict[model_name] = fi_df\n        print(f\"\\n[{model_name}] 상위 10개 Feature Importance:\")\n        print(fi_df.head(10).to_string(index=False))",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZFaEywZH6mei",
    "outputId": "07aa3fbb-1ed5-4c58-839e-4eea2b50ae1e"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 9. 모델 성능 비교\n# ============================================\n\nmetrics_df = pd.DataFrame(metrics_list)\nprint(\"\\n\" + \"=\" * 50)\nprint(\"모델 성능 비교\")\nprint(\"=\" * 50)\ndisplay(metrics_df)\n\n# RMSE 기준 정렬\nmetrics_df_sorted = metrics_df.sort_values(\"RMSE\")\nprint(\"\\n[RMSE 기준 정렬]\")\ndisplay(metrics_df_sorted)",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401
    },
    "id": "gD8Hp8Xh8xHa",
    "outputId": "9f633fb4-9406-4b96-9633-8f646b7df712"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 10. 성능 시각화: RMSE 비교\n# ============================================\n\nplt.figure(figsize=(8, 4))\nsns.barplot(data=metrics_df_sorted, x=\"model\", y=\"RMSE\", palette=\"Blues_d\")\nplt.title(\"모델별 RMSE 비교 (낮을수록 좋음)\")\nplt.xlabel(\"모델\")\nplt.ylabel(\"RMSE\")\nplt.tight_layout()\nplt.show()",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401
    },
    "id": "FAPquH3m8jhU",
    "outputId": "be3af33e-008a-41b1-a49f-689ebbafefe7"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# ============================================\n# 11. 성능 시각화: R² 비교\n# ============================================\n\nplt.figure(figsize=(8, 4))\nsns.barplot(\n    data=metrics_df.sort_values(\"R2\", ascending=False), \n    x=\"model\", y=\"R2\", palette=\"Reds_d\"\n)\nplt.title(\"모델별 R² 비교 (1에 가까울수록 좋음)\")\nplt.xlabel(\"모델\")\nplt.ylabel(\"R²\")\nplt.axhline(y=0, color=\"gray\", linestyle=\"--\", linewidth=1)\nplt.tight_layout()\nplt.show()",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "id": "vWWDDGAv8lKh",
    "outputId": "19013a16-3a2f-462a-8b1b-ea44a3077387"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "# 12. 결과 해석\n\n## 12.1. 성능 분석\n\n모든 모델의 R²가 **음수**로 나타났습니다.\n\n| 지표 | 의미 |\n|------|------|\n| R² < 0 | 모델이 평균값으로만 예측하는 baseline보다 못함 |\n| R² = 0 | baseline과 동일한 성능 |\n| R² = 1 | 완벽한 예측 |\n\n## 12.2. 왜 이런 결과가 나왔는가?\n\n**핵심 원인**: 사고심각도 = 사망 × 3 + 부상\n\n이번 실습에서는 **사망·부상 컬럼을 입력에서 제외**했습니다.\n즉, 타깃을 직접 설명하는 핵심 변수를 제거한 상태입니다.\n\n남은 변수들(시간, 노선, 원인 등)은:\n- 사고 발생 여부와는 관련이 있지만\n- 사망/부상 발생 여부를 정확히 예측하기에는 **정보가 부족**합니다\n\n## 12.3. 이 실습에서 배울 점\n\n1. **데이터 누설(Leakage) 이해**: 타깃과 직접 관련된 변수를 포함하면 성능이 높아지지만, 실제 활용 시에는 의미가 없음\n2. **모델 한계 인식**: 아무리 좋은 모델도 예측에 필요한 정보가 없으면 성능이 낮음\n3. **R² 음수 해석**: 모델이 baseline보다 못하다는 의미로, 피처 선택/타깃 정의를 재검토해야 함\n\n## 12.4. 개선 방향\n\n다음 노트북에서는 **사고발생이정**을 타깃으로 예측합니다.\n- 사고발생이정은 노선명, 방향 등과 구조적 관계가 있어 예측 가능성이 높음\n- 이를 통해 \"예측 가능한 타깃\"과 \"예측 어려운 타깃\"의 차이를 체험",
   "metadata": {
    "id": "oPFyBu0l9_Wp"
   }
  }
 ]
}