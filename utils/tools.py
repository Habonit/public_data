"""
Data analysis tools for Tool Calling in Claude chatbot.

ì´ ëª¨ë“ˆì€ Claude APIì˜ Tool Use ê¸°ëŠ¥ì„ ìœ„í•œ 20ê°œ ë°ì´í„° ë¶„ì„ ë„êµ¬ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
ê° ë„êµ¬ëŠ” pandas DataFrameì„ ë¶„ì„í•˜ì—¬ ê²°ê³¼ë¥¼ ë¬¸ìì—´ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.

v1.1.2: 5ê°œ ì¶”ê°€ ë„êµ¬
- analyze_missing_pattern: ê²°ì¸¡ê°’ íŒ¨í„´ ë¶„ì„ (MCAR, MAR, MNAR)
- get_column_correlation_with_target: íƒ€ê²Ÿ ì»¬ëŸ¼ê³¼ì˜ ìƒê´€ê´€ê³„ ë¶„ì„
- detect_data_types: ì»¬ëŸ¼ë³„ ì‹¤ì œ ë°ì´í„° íƒ€ì… ì¶”ë¡ 
- get_temporal_pattern: ì‹œê°„ ê´€ë ¨ ì»¬ëŸ¼ì˜ íŒ¨í„´ ë¶„ì„
- summarize_categorical_distribution: ë²”ì£¼í˜• ì»¬ëŸ¼ ë¶„í¬ ìš”ì•½
"""
import pandas as pd
import numpy as np
from typing import Any

from utils.geo import detect_lat_lng_columns


# ============================================================================
# Tool Definitions (JSON Schema for Anthropic API)
# ============================================================================

TOOLS = [
    {
        "name": "get_dataframe_info",
        "description": "DataFrame ê¸°ë³¸ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. í–‰/ì—´ ìˆ˜, ì»¬ëŸ¼ëª…, ë°ì´í„° íƒ€ì…ì„ í¬í•¨í•©ë‹ˆë‹¤.",
        "input_schema": {
            "type": "object",
            "properties": {},
            "required": []
        }
    },
    {
        "name": "get_column_statistics",
        "description": "íŠ¹ì • ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì˜ í†µê³„ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. í‰ê· , ì¤‘ì•™ê°’, í‘œì¤€í¸ì°¨, ìµœì†Œ/ìµœëŒ€ê°’ ë“±ì„ í¬í•¨í•©ë‹ˆë‹¤.",
        "input_schema": {
            "type": "object",
            "properties": {
                "column": {
                    "type": "string",
                    "description": "í†µê³„ë¥¼ ê³„ì‚°í•  ì»¬ëŸ¼ëª…"
                }
            },
            "required": ["column"]
        }
    },
    {
        "name": "get_missing_values",
        "description": "ê° ì»¬ëŸ¼ë³„ ê²°ì¸¡ì¹˜ ê°œìˆ˜ì™€ ë¹„ìœ¨ì„ ë¶„ì„í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.",
        "input_schema": {
            "type": "object",
            "properties": {},
            "required": []
        }
    },
    {
        "name": "get_value_counts",
        "description": "ë²”ì£¼í˜• ì»¬ëŸ¼ì˜ ê°’ë³„ ê°œìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. ìƒìœ„ Nê°œë§Œ í‘œì‹œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
        "input_schema": {
            "type": "object",
            "properties": {
                "column": {
                    "type": "string",
                    "description": "ê°’ ë¶„í¬ë¥¼ í™•ì¸í•  ì»¬ëŸ¼ëª…"
                },
                "top_n": {
                    "type": "integer",
                    "description": "ìƒìœ„ Nê°œë§Œ í‘œì‹œ (ê¸°ë³¸ê°’: 20)"
                }
            },
            "required": ["column"]
        }
    },
    {
        "name": "filter_dataframe",
        "description": "ì£¼ì–´ì§„ ì¡°ê±´ì— ë§ëŠ” í–‰ë§Œ í•„í„°ë§í•˜ì—¬ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.",
        "input_schema": {
            "type": "object",
            "properties": {
                "column": {
                    "type": "string",
                    "description": "í•„í„°ë§í•  ì»¬ëŸ¼ëª…"
                },
                "operator": {
                    "type": "string",
                    "enum": ["==", "!=", ">", "<", ">=", "<=", "contains"],
                    "description": "ë¹„êµ ì—°ì‚°ì"
                },
                "value": {
                    "description": "ë¹„êµí•  ê°’"
                }
            },
            "required": ["column", "operator", "value"]
        }
    },
    {
        "name": "sort_dataframe",
        "description": "íŠ¹ì • ì»¬ëŸ¼ì„ ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì •ë ¬í•˜ì—¬ ìƒìœ„ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.",
        "input_schema": {
            "type": "object",
            "properties": {
                "column": {
                    "type": "string",
                    "description": "ì •ë ¬ ê¸°ì¤€ ì»¬ëŸ¼ëª…"
                },
                "ascending": {
                    "type": "boolean",
                    "description": "ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬ ì—¬ë¶€ (ê¸°ë³¸ê°’: true)"
                },
                "top_n": {
                    "type": "integer",
                    "description": "ë°˜í™˜í•  í–‰ ìˆ˜ (ê¸°ë³¸ê°’: 10)"
                }
            },
            "required": ["column"]
        }
    },
    {
        "name": "get_correlation",
        "description": "ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë“¤ ê°„ì˜ ìƒê´€ê´€ê³„ë¥¼ ë¶„ì„í•˜ì—¬ ìƒê´€ê³„ìˆ˜ í–‰ë ¬ì„ ë°˜í™˜í•©ë‹ˆë‹¤.",
        "input_schema": {
            "type": "object",
            "properties": {
                "columns": {
                    "type": "array",
                    "items": {"type": "string"},
                    "description": "ìƒê´€ê´€ê³„ë¥¼ ë¶„ì„í•  ì»¬ëŸ¼ ëª©ë¡ (ë¹„ì–´ìˆìœ¼ë©´ ëª¨ë“  ìˆ˜ì¹˜í˜• ì»¬ëŸ¼)"
                }
            },
            "required": []
        }
    },
    {
        "name": "group_by_aggregate",
        "description": "íŠ¹ì • ì»¬ëŸ¼ì„ ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹í™”í•˜ê³  ì§‘ê³„ ì—°ì‚°ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.",
        "input_schema": {
            "type": "object",
            "properties": {
                "group_column": {
                    "type": "string",
                    "description": "ê·¸ë£¹í™” ê¸°ì¤€ ì»¬ëŸ¼ëª…"
                },
                "agg_column": {
                    "type": "string",
                    "description": "ì§‘ê³„í•  ì»¬ëŸ¼ëª…"
                },
                "operation": {
                    "type": "string",
                    "enum": ["sum", "mean", "count", "min", "max", "median", "std"],
                    "description": "ì§‘ê³„ ì—°ì‚° ì¢…ë¥˜"
                }
            },
            "required": ["group_column", "agg_column", "operation"]
        }
    },
    {
        "name": "get_unique_values",
        "description": "íŠ¹ì • ì»¬ëŸ¼ì˜ ê³ ìœ ê°’ ëª©ë¡ê³¼ ê°œìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.",
        "input_schema": {
            "type": "object",
            "properties": {
                "column": {
                    "type": "string",
                    "description": "ê³ ìœ ê°’ì„ í™•ì¸í•  ì»¬ëŸ¼ëª…"
                }
            },
            "required": ["column"]
        }
    },
    {
        "name": "get_date_range",
        "description": "ë‚ ì§œ ì»¬ëŸ¼ì˜ ìµœì†Œ/ìµœëŒ€ ë‚ ì§œì™€ ê¸°ê°„ì„ ë¶„ì„í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.",
        "input_schema": {
            "type": "object",
            "properties": {
                "column": {
                    "type": "string",
                    "description": "ë‚ ì§œ ì»¬ëŸ¼ëª…"
                }
            },
            "required": ["column"]
        }
    },
    {
        "name": "get_outliers",
        "description": "IQR(ì‚¬ë¶„ìœ„ìˆ˜ ë²”ìœ„) ê¸°ë°˜ìœ¼ë¡œ ì´ìƒì¹˜ë¥¼ íƒì§€í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.",
        "input_schema": {
            "type": "object",
            "properties": {
                "column": {
                    "type": "string",
                    "description": "ì´ìƒì¹˜ë¥¼ íƒì§€í•  ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ëª…"
                },
                "multiplier": {
                    "type": "number",
                    "description": "IQR ë°°ìˆ˜ (ê¸°ë³¸ê°’: 1.5)"
                }
            },
            "required": ["column"]
        }
    },
    {
        "name": "get_sample_rows",
        "description": "ë°ì´í„°ì—ì„œ ìƒ˜í”Œ í–‰ì„ ì¶”ì¶œí•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤. ì¡°ê±´ì„ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
        "input_schema": {
            "type": "object",
            "properties": {
                "n": {
                    "type": "integer",
                    "description": "ì¶”ì¶œí•  ìƒ˜í”Œ ìˆ˜ (ê¸°ë³¸ê°’: 5)"
                },
                "column": {
                    "type": "string",
                    "description": "ì¡°ê±´ì„ ì ìš©í•  ì»¬ëŸ¼ëª… (ì„ íƒ)"
                },
                "value": {
                    "description": "í•„í„°ë§í•  ê°’ (ì„ íƒ)"
                }
            },
            "required": []
        }
    },
    {
        "name": "calculate_percentile",
        "description": "ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì—ì„œ íŠ¹ì • ë°±ë¶„ìœ„ìˆ˜ ê°’ì„ ê³„ì‚°í•©ë‹ˆë‹¤.",
        "input_schema": {
            "type": "object",
            "properties": {
                "column": {
                    "type": "string",
                    "description": "ë°±ë¶„ìœ„ìˆ˜ë¥¼ ê³„ì‚°í•  ì»¬ëŸ¼ëª…"
                },
                "percentile": {
                    "type": "number",
                    "description": "ê³„ì‚°í•  ë°±ë¶„ìœ„ìˆ˜ (0-100)"
                }
            },
            "required": ["column", "percentile"]
        }
    },
    {
        "name": "get_geo_bounds",
        "description": "ìœ„ê²½ë„ ë°ì´í„°ì˜ ì§€ë¦¬ì  ë²”ìœ„(ìµœì†Œ/ìµœëŒ€ ìœ„ë„, ê²½ë„)ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.",
        "input_schema": {
            "type": "object",
            "properties": {},
            "required": []
        }
    },
    {
        "name": "cross_tabulation",
        "description": "ë‘ ë²”ì£¼í˜• ì»¬ëŸ¼ ê°„ì˜ êµì°¨í‘œ(ë¹ˆë„í‘œ)ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.",
        "input_schema": {
            "type": "object",
            "properties": {
                "row_column": {
                    "type": "string",
                    "description": "í–‰(row)ìœ¼ë¡œ ì‚¬ìš©í•  ì»¬ëŸ¼ëª…"
                },
                "col_column": {
                    "type": "string",
                    "description": "ì—´(column)ìœ¼ë¡œ ì‚¬ìš©í•  ì»¬ëŸ¼ëª…"
                },
                "normalize": {
                    "type": "boolean",
                    "description": "ë¹„ìœ¨ë¡œ ì •ê·œí™” ì—¬ë¶€ (ê¸°ë³¸ê°’: false)"
                }
            },
            "required": ["row_column", "col_column"]
        }
    },
    # v1.1.2: 5ê°œ ì¶”ê°€ ë¶„ì„ ë„êµ¬
    {
        "name": "analyze_missing_pattern",
        "description": "ê²°ì¸¡ê°’ íŒ¨í„´ì„ ë¶„ì„í•˜ì—¬ MCAR, MAR, MNAR ì—¬ë¶€ë¥¼ ì¶”ì •í•©ë‹ˆë‹¤. ê²°ì¸¡ê°’ì´ ë°œìƒí•œ ì›ì¸ê³¼ íŒ¨í„´ì„ íŒŒì•…í•©ë‹ˆë‹¤.",
        "input_schema": {
            "type": "object",
            "properties": {
                "column": {
                    "type": "string",
                    "description": "ê²°ì¸¡ê°’ íŒ¨í„´ì„ ë¶„ì„í•  ì»¬ëŸ¼ëª…"
                }
            },
            "required": ["column"]
        }
    },
    {
        "name": "get_column_correlation_with_target",
        "description": "íŠ¹ì • íƒ€ê²Ÿ ì»¬ëŸ¼ê³¼ ë‹¤ë¥¸ ëª¨ë“  ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë“¤ ê°„ì˜ ìƒê´€ê´€ê³„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.",
        "input_schema": {
            "type": "object",
            "properties": {
                "target_column": {
                    "type": "string",
                    "description": "íƒ€ê²Ÿ ì»¬ëŸ¼ëª…"
                }
            },
            "required": ["target_column"]
        }
    },
    {
        "name": "detect_data_types",
        "description": "ì»¬ëŸ¼ë³„ ì‹¤ì œ ë°ì´í„° íƒ€ì…ì„ ì¶”ë¡ í•©ë‹ˆë‹¤. ìˆ«ìì²˜ëŸ¼ ë³´ì´ëŠ” ë¬¸ìì—´, ë‚ ì§œ í˜•ì‹ ë“±ì„ ê°ì§€í•©ë‹ˆë‹¤.",
        "input_schema": {
            "type": "object",
            "properties": {},
            "required": []
        }
    },
    {
        "name": "get_temporal_pattern",
        "description": "ì‹œê°„/ë‚ ì§œ ê´€ë ¨ ì»¬ëŸ¼ì˜ íŒ¨í„´ì„ ë¶„ì„í•©ë‹ˆë‹¤. ì›”ë³„, ìš”ì¼ë³„, ì‹œê°„ëŒ€ë³„ ë¶„í¬ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.",
        "input_schema": {
            "type": "object",
            "properties": {
                "column": {
                    "type": "string",
                    "description": "ì‹œê°„/ë‚ ì§œ ì»¬ëŸ¼ëª…"
                }
            },
            "required": ["column"]
        }
    },
    {
        "name": "summarize_categorical_distribution",
        "description": "ë²”ì£¼í˜• ì»¬ëŸ¼ì˜ ë¶„í¬ë¥¼ ìƒì„¸í•˜ê²Œ ìš”ì•½í•©ë‹ˆë‹¤. ì§‘ì¤‘ë„, í¸í–¥ì„±, í¬ê·€ ì¹´í…Œê³ ë¦¬ ë“±ì„ ë¶„ì„í•©ë‹ˆë‹¤.",
        "input_schema": {
            "type": "object",
            "properties": {
                "column": {
                    "type": "string",
                    "description": "ë²”ì£¼í˜• ì»¬ëŸ¼ëª…"
                }
            },
            "required": ["column"]
        }
    }
]


# ============================================================================
# Tool Handlers (T011-T025)
# ============================================================================

def get_dataframe_info(df: pd.DataFrame, **kwargs) -> str:
    """
    DataFrame ê¸°ë³¸ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

    Parameters:
        df (pd.DataFrame): ë¶„ì„í•  DataFrame

    Returns:
        str: DataFrame ì •ë³´ ë¬¸ìì—´
    """
    if df.empty:
        return "ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤ (ë¹ˆ DataFrame)."

    info_lines = [
        f"## DataFrame ê¸°ë³¸ ì •ë³´",
        f"- í–‰ ìˆ˜: {len(df):,}",
        f"- ì—´ ìˆ˜: {len(df.columns)}",
        f"",
        f"## ì»¬ëŸ¼ ëª©ë¡ ë° ë°ì´í„° íƒ€ì…",
    ]

    for col in df.columns:
        dtype = str(df[col].dtype)
        non_null = df[col].notna().sum()
        info_lines.append(f"- {col}: {dtype} (ë¹„ê²°ì¸¡ì¹˜: {non_null:,})")

    return "\n".join(info_lines)


def get_column_statistics(df: pd.DataFrame, column: str, **kwargs) -> str:
    """
    íŠ¹ì • ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì˜ í†µê³„ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

    Parameters:
        df (pd.DataFrame): ë¶„ì„í•  DataFrame
        column (str): í†µê³„ë¥¼ ê³„ì‚°í•  ì»¬ëŸ¼ëª…

    Returns:
        str: í†µê³„ ì •ë³´ ë¬¸ìì—´
    """
    if column not in df.columns:
        return f"'{column}' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {', '.join(df.columns)}"

    if not pd.api.types.is_numeric_dtype(df[column]):
        return f"'{column}' ì»¬ëŸ¼ì€ ìˆ˜ì¹˜í˜•ì´ ì•„ë‹™ë‹ˆë‹¤. ë°ì´í„° íƒ€ì…: {df[column].dtype}"

    col_data = df[column].dropna()

    if len(col_data) == 0:
        return f"'{column}' ì»¬ëŸ¼ì˜ ëª¨ë“  ê°’ì´ ê²°ì¸¡ì¹˜ì…ë‹ˆë‹¤."

    stats = {
        "ê°œìˆ˜": len(col_data),
        "í‰ê· ": col_data.mean(),
        "í‘œì¤€í¸ì°¨": col_data.std(),
        "ìµœì†Œê°’": col_data.min(),
        "25%": col_data.quantile(0.25),
        "ì¤‘ì•™ê°’": col_data.median(),
        "75%": col_data.quantile(0.75),
        "ìµœëŒ€ê°’": col_data.max(),
    }

    lines = [f"## '{column}' ì»¬ëŸ¼ í†µê³„"]
    for name, value in stats.items():
        if isinstance(value, float):
            lines.append(f"- {name}: {value:,.2f}")
        else:
            lines.append(f"- {name}: {value:,}")

    return "\n".join(lines)


def get_missing_values(df: pd.DataFrame, **kwargs) -> str:
    """
    ê° ì»¬ëŸ¼ë³„ ê²°ì¸¡ì¹˜ ê°œìˆ˜ì™€ ë¹„ìœ¨ì„ ë¶„ì„í•©ë‹ˆë‹¤.

    Parameters:
        df (pd.DataFrame): ë¶„ì„í•  DataFrame

    Returns:
        str: ê²°ì¸¡ì¹˜ í˜„í™© ë¬¸ìì—´
    """
    if df.empty:
        return "ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤ (ë¹ˆ DataFrame)."

    total_rows = len(df)
    lines = [f"## ê²°ì¸¡ì¹˜ í˜„í™© (ì „ì²´ {total_rows:,}í–‰)"]

    for col in df.columns:
        missing_count = df[col].isnull().sum()
        missing_pct = (missing_count / total_rows * 100) if total_rows > 0 else 0
        lines.append(f"- {col}: {missing_count:,}ê°œ ({missing_pct:.1f}%)")

    total_missing = df.isnull().sum().sum()
    total_cells = total_rows * len(df.columns)
    total_pct = (total_missing / total_cells * 100) if total_cells > 0 else 0
    lines.append(f"\n**ì „ì²´ ê²°ì¸¡ì¹˜**: {total_missing:,}ê°œ / {total_cells:,}ê°œ ({total_pct:.1f}%)")

    return "\n".join(lines)


def get_value_counts(df: pd.DataFrame, column: str, top_n: int = 20, **kwargs) -> str:
    """
    ë²”ì£¼í˜• ì»¬ëŸ¼ì˜ ê°’ë³„ ê°œìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

    Parameters:
        df (pd.DataFrame): ë¶„ì„í•  DataFrame
        column (str): ê°’ ë¶„í¬ë¥¼ í™•ì¸í•  ì»¬ëŸ¼ëª…
        top_n (int): ìƒìœ„ Nê°œë§Œ í‘œì‹œ (ê¸°ë³¸ê°’: 20)

    Returns:
        str: ê°’ ë¶„í¬ ë¬¸ìì—´
    """
    if column not in df.columns:
        return f"'{column}' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {', '.join(df.columns)}"

    value_counts = df[column].value_counts()
    total_unique = len(value_counts)

    lines = [f"## '{column}' ì»¬ëŸ¼ ê°’ ë¶„í¬ (ìƒìœ„ {min(top_n, total_unique)}ê°œ / ì´ {total_unique}ê°œ)"]

    for idx, (value, count) in enumerate(value_counts.head(top_n).items()):
        pct = count / len(df) * 100
        lines.append(f"- {value}: {count:,}ê°œ ({pct:.1f}%)")

    if total_unique > top_n:
        lines.append(f"\n... ì™¸ {total_unique - top_n}ê°œ ê°’")

    return "\n".join(lines)


def filter_dataframe(df: pd.DataFrame, column: str, operator: str, value: Any, **kwargs) -> str:
    """
    ì¡°ê±´ì— ë§ëŠ” í–‰ë§Œ í•„í„°ë§í•˜ì—¬ ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

    Parameters:
        df (pd.DataFrame): ë¶„ì„í•  DataFrame
        column (str): í•„í„°ë§í•  ì»¬ëŸ¼ëª…
        operator (str): ë¹„êµ ì—°ì‚°ì
        value: ë¹„êµí•  ê°’

    Returns:
        str: í•„í„°ë§ ê²°ê³¼ ë¬¸ìì—´
    """
    if column not in df.columns:
        return f"'{column}' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {', '.join(df.columns)}"

    try:
        if operator == "==":
            filtered = df[df[column] == value]
        elif operator == "!=":
            filtered = df[df[column] != value]
        elif operator == ">":
            filtered = df[df[column] > value]
        elif operator == "<":
            filtered = df[df[column] < value]
        elif operator == ">=":
            filtered = df[df[column] >= value]
        elif operator == "<=":
            filtered = df[df[column] <= value]
        elif operator == "contains":
            filtered = df[df[column].astype(str).str.contains(str(value), case=False, na=False)]
        else:
            return f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì—°ì‚°ìì…ë‹ˆë‹¤: {operator}"

        lines = [
            f"## í•„í„°ë§ ê²°ê³¼: {column} {operator} {value}",
            f"- ì›ë³¸ í–‰ ìˆ˜: {len(df):,}",
            f"- í•„í„°ë§ í›„ í–‰ ìˆ˜: {len(filtered):,}",
            f"",
            f"### ìƒ˜í”Œ ë°ì´í„° (ìµœëŒ€ 10í–‰)",
            filtered.head(10).to_string(index=False)
        ]

        return "\n".join(lines)

    except Exception as e:
        return f"í•„í„°ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


def sort_dataframe(df: pd.DataFrame, column: str, ascending: bool = True, top_n: int = 10, **kwargs) -> str:
    """
    íŠ¹ì • ì»¬ëŸ¼ì„ ê¸°ì¤€ìœ¼ë¡œ ë°ì´í„°ë¥¼ ì •ë ¬í•©ë‹ˆë‹¤.

    Parameters:
        df (pd.DataFrame): ë¶„ì„í•  DataFrame
        column (str): ì •ë ¬ ê¸°ì¤€ ì»¬ëŸ¼ëª…
        ascending (bool): ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬ ì—¬ë¶€
        top_n (int): ë°˜í™˜í•  í–‰ ìˆ˜

    Returns:
        str: ì •ë ¬ ê²°ê³¼ ë¬¸ìì—´
    """
    if column not in df.columns:
        return f"'{column}' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {', '.join(df.columns)}"

    try:
        sorted_df = df.sort_values(by=column, ascending=ascending)
        order_text = "ì˜¤ë¦„ì°¨ìˆœ" if ascending else "ë‚´ë¦¼ì°¨ìˆœ"

        lines = [
            f"## ì •ë ¬ ê²°ê³¼: {column} ê¸°ì¤€ ({order_text})",
            f"- ìƒìœ„ {min(top_n, len(sorted_df))}í–‰",
            f"",
            sorted_df.head(top_n).to_string(index=False)
        ]

        return "\n".join(lines)

    except Exception as e:
        return f"ì •ë ¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


def get_correlation(df: pd.DataFrame, columns: list[str] | None = None, **kwargs) -> str:
    """
    ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë“¤ ê°„ì˜ ìƒê´€ê´€ê³„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.

    Parameters:
        df (pd.DataFrame): ë¶„ì„í•  DataFrame
        columns (list[str] | None): ë¶„ì„í•  ì»¬ëŸ¼ ëª©ë¡

    Returns:
        str: ìƒê´€ê³„ìˆ˜ í–‰ë ¬ ë¬¸ìì—´
    """
    numeric_df = df.select_dtypes(include=[np.number])

    if numeric_df.empty:
        return "ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤."

    if columns:
        valid_columns = [c for c in columns if c in numeric_df.columns]
        if not valid_columns:
            return f"ì§€ì •í•œ ì»¬ëŸ¼ ì¤‘ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤. ìˆ˜ì¹˜í˜• ì»¬ëŸ¼: {', '.join(numeric_df.columns)}"
        numeric_df = numeric_df[valid_columns]

    if len(numeric_df.columns) < 2:
        return "ìƒê´€ê´€ê³„ ë¶„ì„ì—ëŠ” ìµœì†Œ 2ê°œì˜ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤."

    corr_matrix = numeric_df.corr()

    lines = [
        f"## ìƒê´€ê³„ìˆ˜ í–‰ë ¬ ({len(corr_matrix.columns)}ê°œ ì»¬ëŸ¼)",
        f"",
        corr_matrix.round(3).to_string()
    ]

    return "\n".join(lines)


def group_by_aggregate(df: pd.DataFrame, group_column: str, agg_column: str, operation: str, **kwargs) -> str:
    """
    íŠ¹ì • ì»¬ëŸ¼ì„ ê¸°ì¤€ìœ¼ë¡œ ê·¸ë£¹í™”í•˜ê³  ì§‘ê³„ ì—°ì‚°ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

    Parameters:
        df (pd.DataFrame): ë¶„ì„í•  DataFrame
        group_column (str): ê·¸ë£¹í™” ê¸°ì¤€ ì»¬ëŸ¼ëª…
        agg_column (str): ì§‘ê³„í•  ì»¬ëŸ¼ëª…
        operation (str): ì§‘ê³„ ì—°ì‚° ì¢…ë¥˜

    Returns:
        str: ê·¸ë£¹ë³„ ì§‘ê³„ ê²°ê³¼ ë¬¸ìì—´
    """
    if group_column not in df.columns:
        return f"'{group_column}' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    if agg_column not in df.columns:
        return f"'{agg_column}' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    valid_ops = ["sum", "mean", "count", "min", "max", "median", "std"]
    if operation not in valid_ops:
        return f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì§‘ê³„ ì—°ì‚°ì…ë‹ˆë‹¤: {operation}. ì§€ì›: {', '.join(valid_ops)}"

    try:
        if operation == "count":
            result = df.groupby(group_column)[agg_column].count()
        else:
            result = df.groupby(group_column)[agg_column].agg(operation)

        result_df = result.reset_index()
        result_df.columns = [group_column, f"{agg_column}_{operation}"]

        op_korean = {
            "sum": "í•©ê³„", "mean": "í‰ê· ", "count": "ê°œìˆ˜",
            "min": "ìµœì†Œ", "max": "ìµœëŒ€", "median": "ì¤‘ì•™ê°’", "std": "í‘œì¤€í¸ì°¨"
        }

        lines = [
            f"## ê·¸ë£¹ë³„ ì§‘ê³„: {group_column}ë³„ {agg_column} {op_korean.get(operation, operation)}",
            f"- ê·¸ë£¹ ìˆ˜: {len(result_df)}",
            f"",
            result_df.to_string(index=False)
        ]

        return "\n".join(lines)

    except Exception as e:
        return f"ì§‘ê³„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


def get_unique_values(df: pd.DataFrame, column: str, **kwargs) -> str:
    """
    íŠ¹ì • ì»¬ëŸ¼ì˜ ê³ ìœ ê°’ ëª©ë¡ê³¼ ê°œìˆ˜ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

    Parameters:
        df (pd.DataFrame): ë¶„ì„í•  DataFrame
        column (str): ê³ ìœ ê°’ì„ í™•ì¸í•  ì»¬ëŸ¼ëª…

    Returns:
        str: ê³ ìœ ê°’ ëª©ë¡ ë¬¸ìì—´
    """
    if column not in df.columns:
        return f"'{column}' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {', '.join(df.columns)}"

    unique_values = df[column].dropna().unique()
    unique_count = len(unique_values)

    lines = [f"## '{column}' ì»¬ëŸ¼ ê³ ìœ ê°’ ({unique_count}ê°œ)"]

    if unique_count <= 50:
        for val in sorted(unique_values, key=lambda x: str(x)):
            lines.append(f"- {val}")
    else:
        lines.append(f"ê³ ìœ ê°’ì´ ë„ˆë¬´ ë§ì•„ ì²˜ìŒ 50ê°œë§Œ í‘œì‹œí•©ë‹ˆë‹¤:")
        for val in sorted(unique_values, key=lambda x: str(x))[:50]:
            lines.append(f"- {val}")
        lines.append(f"... ì™¸ {unique_count - 50}ê°œ")

    return "\n".join(lines)


def get_date_range(df: pd.DataFrame, column: str, **kwargs) -> str:
    """
    ë‚ ì§œ ì»¬ëŸ¼ì˜ ìµœì†Œ/ìµœëŒ€ ë‚ ì§œì™€ ê¸°ê°„ì„ ë¶„ì„í•©ë‹ˆë‹¤.

    Parameters:
        df (pd.DataFrame): ë¶„ì„í•  DataFrame
        column (str): ë‚ ì§œ ì»¬ëŸ¼ëª…

    Returns:
        str: ë‚ ì§œ ë²”ìœ„ ì •ë³´ ë¬¸ìì—´
    """
    if column not in df.columns:
        return f"'{column}' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    try:
        date_col = pd.to_datetime(df[column], errors='coerce')
        valid_dates = date_col.dropna()

        if len(valid_dates) == 0:
            return f"'{column}' ì»¬ëŸ¼ì— ìœ íš¨í•œ ë‚ ì§œê°€ ì—†ìŠµë‹ˆë‹¤."

        min_date = valid_dates.min()
        max_date = valid_dates.max()
        date_range = max_date - min_date

        lines = [
            f"## '{column}' ì»¬ëŸ¼ ë‚ ì§œ ë²”ìœ„",
            f"- ì‹œì‘ ë‚ ì§œ: {min_date.strftime('%Y-%m-%d')}",
            f"- ì¢…ë£Œ ë‚ ì§œ: {max_date.strftime('%Y-%m-%d')}",
            f"- ê¸°ê°„: {date_range.days}ì¼",
            f"- ìœ íš¨ ë‚ ì§œ ìˆ˜: {len(valid_dates):,}ê°œ",
            f"- ê²°ì¸¡ ë‚ ì§œ ìˆ˜: {len(date_col) - len(valid_dates):,}ê°œ"
        ]

        return "\n".join(lines)

    except Exception as e:
        return f"ë‚ ì§œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


def get_outliers(df: pd.DataFrame, column: str, multiplier: float = 1.5, **kwargs) -> str:
    """
    IQR ê¸°ë°˜ìœ¼ë¡œ ì´ìƒì¹˜ë¥¼ íƒì§€í•©ë‹ˆë‹¤.

    Parameters:
        df (pd.DataFrame): ë¶„ì„í•  DataFrame
        column (str): ì´ìƒì¹˜ë¥¼ íƒì§€í•  ì»¬ëŸ¼ëª…
        multiplier (float): IQR ë°°ìˆ˜ (ê¸°ë³¸ê°’: 1.5)

    Returns:
        str: ì´ìƒì¹˜ ì •ë³´ ë¬¸ìì—´
    """
    if column not in df.columns:
        return f"'{column}' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    if not pd.api.types.is_numeric_dtype(df[column]):
        return f"'{column}' ì»¬ëŸ¼ì€ ìˆ˜ì¹˜í˜•ì´ ì•„ë‹™ë‹ˆë‹¤."

    col_data = df[column].dropna()

    if len(col_data) == 0:
        return f"'{column}' ì»¬ëŸ¼ì˜ ëª¨ë“  ê°’ì´ ê²°ì¸¡ì¹˜ì…ë‹ˆë‹¤."

    q1 = col_data.quantile(0.25)
    q3 = col_data.quantile(0.75)
    iqr = q3 - q1

    lower_bound = q1 - multiplier * iqr
    upper_bound = q3 + multiplier * iqr

    outliers_low = col_data[col_data < lower_bound]
    outliers_high = col_data[col_data > upper_bound]
    total_outliers = len(outliers_low) + len(outliers_high)

    lines = [
        f"## '{column}' ì»¬ëŸ¼ ì´ìƒì¹˜ ë¶„ì„ (IQR ë°°ìˆ˜: {multiplier})",
        f"",
        f"### ê¸°ì¤€ê°’",
        f"- Q1 (25%): {q1:,.2f}",
        f"- Q3 (75%): {q3:,.2f}",
        f"- IQR: {iqr:,.2f}",
        f"- í•˜í•œì„ : {lower_bound:,.2f}",
        f"- ìƒí•œì„ : {upper_bound:,.2f}",
        f"",
        f"### ì´ìƒì¹˜ í˜„í™©",
        f"- í•˜í•œ ë¯¸ë§Œ: {len(outliers_low)}ê°œ",
        f"- ìƒí•œ ì´ˆê³¼: {len(outliers_high)}ê°œ",
        f"- ì´ ì´ìƒì¹˜: {total_outliers}ê°œ ({total_outliers/len(col_data)*100:.1f}%)"
    ]

    return "\n".join(lines)


def get_sample_rows(df: pd.DataFrame, n: int = 5, column: str | None = None, value: Any = None, **kwargs) -> str:
    """
    ë°ì´í„°ì—ì„œ ìƒ˜í”Œ í–‰ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.

    Parameters:
        df (pd.DataFrame): ë¶„ì„í•  DataFrame
        n (int): ì¶”ì¶œí•  ìƒ˜í”Œ ìˆ˜
        column (str | None): ì¡°ê±´ì„ ì ìš©í•  ì»¬ëŸ¼ëª…
        value: í•„í„°ë§í•  ê°’

    Returns:
        str: ìƒ˜í”Œ ë°ì´í„° ë¬¸ìì—´
    """
    if df.empty:
        return "ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤ (ë¹ˆ DataFrame)."

    if column and value is not None:
        if column not in df.columns:
            return f"'{column}' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        filtered_df = df[df[column] == value]
        title = f"## ìƒ˜í”Œ ë°ì´í„°: {column} = {value} (ìµœëŒ€ {n}í–‰)"
    else:
        filtered_df = df
        title = f"## ìƒ˜í”Œ ë°ì´í„° (ìµœëŒ€ {n}í–‰)"

    if len(filtered_df) == 0:
        return "ì¡°ê±´ì— ë§ëŠ” ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."

    sample_df = filtered_df.sample(min(n, len(filtered_df)), random_state=42)

    lines = [
        title,
        f"- ì „ì²´ í–‰ ìˆ˜: {len(filtered_df):,}",
        f"",
        sample_df.to_string(index=False)
    ]

    return "\n".join(lines)


def calculate_percentile(df: pd.DataFrame, column: str, percentile: float, **kwargs) -> str:
    """
    ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì—ì„œ íŠ¹ì • ë°±ë¶„ìœ„ìˆ˜ ê°’ì„ ê³„ì‚°í•©ë‹ˆë‹¤.

    Parameters:
        df (pd.DataFrame): ë¶„ì„í•  DataFrame
        column (str): ë°±ë¶„ìœ„ìˆ˜ë¥¼ ê³„ì‚°í•  ì»¬ëŸ¼ëª…
        percentile (float): ê³„ì‚°í•  ë°±ë¶„ìœ„ìˆ˜ (0-100)

    Returns:
        str: ë°±ë¶„ìœ„ìˆ˜ ê²°ê³¼ ë¬¸ìì—´
    """
    if column not in df.columns:
        return f"'{column}' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    if not pd.api.types.is_numeric_dtype(df[column]):
        return f"'{column}' ì»¬ëŸ¼ì€ ìˆ˜ì¹˜í˜•ì´ ì•„ë‹™ë‹ˆë‹¤."

    if not 0 <= percentile <= 100:
        return f"ë°±ë¶„ìœ„ìˆ˜ëŠ” 0-100 ì‚¬ì´ì—¬ì•¼ í•©ë‹ˆë‹¤. ì…ë ¥ê°’: {percentile}"

    col_data = df[column].dropna()

    if len(col_data) == 0:
        return f"'{column}' ì»¬ëŸ¼ì˜ ëª¨ë“  ê°’ì´ ê²°ì¸¡ì¹˜ì…ë‹ˆë‹¤."

    result = col_data.quantile(percentile / 100)

    lines = [
        f"## '{column}' ì»¬ëŸ¼ {percentile}ë²ˆì§¸ ë°±ë¶„ìœ„ìˆ˜",
        f"- ê²°ê³¼ê°’: {result:,.2f}",
        f"- ë°ì´í„° ìˆ˜: {len(col_data):,}ê°œ"
    ]

    return "\n".join(lines)


def get_geo_bounds(df: pd.DataFrame, **kwargs) -> str:
    """
    ìœ„ê²½ë„ ë°ì´í„°ì˜ ì§€ë¦¬ì  ë²”ìœ„ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

    Parameters:
        df (pd.DataFrame): ë¶„ì„í•  DataFrame

    Returns:
        str: ì§€ë¦¬ì  ë²”ìœ„ ë¬¸ìì—´
    """
    lat_col, lng_col = detect_lat_lng_columns(df)

    if not lat_col or not lng_col:
        return "ìœ„ê²½ë„ ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    lat_data = df[lat_col].dropna()
    lng_data = df[lng_col].dropna()

    if len(lat_data) == 0 or len(lng_data) == 0:
        return "ìœ íš¨í•œ ì¢Œí‘œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."

    lines = [
        f"## ì§€ë¦¬ì  ë²”ìœ„",
        f"- ìœ„ë„ ì»¬ëŸ¼: {lat_col}",
        f"- ê²½ë„ ì»¬ëŸ¼: {lng_col}",
        f"",
        f"### ìœ„ë„ ë²”ìœ„",
        f"- ìµœì†Œ: {lat_data.min():.6f}",
        f"- ìµœëŒ€: {lat_data.max():.6f}",
        f"- ë²”ìœ„: {lat_data.max() - lat_data.min():.6f}",
        f"",
        f"### ê²½ë„ ë²”ìœ„",
        f"- ìµœì†Œ: {lng_data.min():.6f}",
        f"- ìµœëŒ€: {lng_data.max():.6f}",
        f"- ë²”ìœ„: {lng_data.max() - lng_data.min():.6f}",
        f"",
        f"- ìœ íš¨ ì¢Œí‘œ ìˆ˜: {min(len(lat_data), len(lng_data)):,}ê°œ"
    ]

    return "\n".join(lines)


def cross_tabulation(df: pd.DataFrame, row_column: str, col_column: str, normalize: bool = False, **kwargs) -> str:
    """
    ë‘ ë²”ì£¼í˜• ì»¬ëŸ¼ ê°„ì˜ êµì°¨í‘œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

    Parameters:
        df (pd.DataFrame): ë¶„ì„í•  DataFrame
        row_column (str): í–‰ìœ¼ë¡œ ì‚¬ìš©í•  ì»¬ëŸ¼ëª…
        col_column (str): ì—´ë¡œ ì‚¬ìš©í•  ì»¬ëŸ¼ëª…
        normalize (bool): ë¹„ìœ¨ë¡œ ì •ê·œí™” ì—¬ë¶€

    Returns:
        str: êµì°¨í‘œ ë¬¸ìì—´
    """
    if row_column not in df.columns:
        return f"'{row_column}' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    if col_column not in df.columns:
        return f"'{col_column}' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    try:
        if normalize:
            cross_tab = pd.crosstab(df[row_column], df[col_column], normalize='all')
            cross_tab = cross_tab.round(3)
        else:
            cross_tab = pd.crosstab(df[row_column], df[col_column])

        normalize_text = " (ë¹„ìœ¨)" if normalize else ""

        lines = [
            f"## êµì°¨í‘œ: {row_column} x {col_column}{normalize_text}",
            f"",
            cross_tab.to_string()
        ]

        return "\n".join(lines)

    except Exception as e:
        return f"êµì°¨í‘œ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


# ============================================================================
# v1.1.2 ì¶”ê°€ ë„êµ¬ í•¸ë“¤ëŸ¬ (5ê°œ)
# ============================================================================

def analyze_missing_pattern(df: pd.DataFrame, column: str, **kwargs) -> str:
    """
    ê²°ì¸¡ê°’ íŒ¨í„´ì„ ë¶„ì„í•˜ì—¬ MCAR, MAR, MNAR ì—¬ë¶€ë¥¼ ì¶”ì •í•©ë‹ˆë‹¤.

    Parameters:
        df (pd.DataFrame): ë¶„ì„í•  DataFrame
        column (str): ê²°ì¸¡ê°’ íŒ¨í„´ì„ ë¶„ì„í•  ì»¬ëŸ¼ëª…

    Returns:
        str: ê²°ì¸¡ê°’ íŒ¨í„´ ë¶„ì„ ê²°ê³¼ ë¬¸ìì—´
    """
    if column not in df.columns:
        return f"'{column}' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    missing_mask = df[column].isnull()
    total_rows = len(df)
    missing_count = missing_mask.sum()
    missing_pct = (missing_count / total_rows * 100) if total_rows > 0 else 0

    if missing_count == 0:
        return f"'{column}' ì»¬ëŸ¼ì— ê²°ì¸¡ê°’ì´ ì—†ìŠµë‹ˆë‹¤."

    lines = [
        f"## '{column}' ì»¬ëŸ¼ ê²°ì¸¡ê°’ íŒ¨í„´ ë¶„ì„",
        f"",
        f"### ê¸°ë³¸ í˜„í™©",
        f"- ì „ì²´ í–‰ ìˆ˜: {total_rows:,}",
        f"- ê²°ì¸¡ê°’ ìˆ˜: {missing_count:,} ({missing_pct:.1f}%)",
        f"",
        f"### ê²°ì¸¡ê°’ íŒ¨í„´ ì¶”ì •"
    ]

    # ë‹¤ë¥¸ ì»¬ëŸ¼ë“¤ê³¼ì˜ ê´€ê³„ ë¶„ì„
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    if column in numeric_cols:
        numeric_cols.remove(column)

    correlations = []
    for other_col in numeric_cols[:5]:  # ìµœëŒ€ 5ê°œ ì»¬ëŸ¼ë§Œ ë¶„ì„
        # ê²°ì¸¡ ì—¬ë¶€ì™€ ë‹¤ë¥¸ ì»¬ëŸ¼ ê°’ ê°„ì˜ ìƒê´€ê´€ê³„
        valid_mask = df[other_col].notna()
        if valid_mask.sum() > 10:
            missing_indicator = missing_mask.astype(int)
            corr = df.loc[valid_mask, [other_col]].assign(missing=missing_indicator[valid_mask])
            r = corr['missing'].corr(corr[other_col])
            if not np.isnan(r):
                correlations.append((other_col, abs(r)))

    if correlations:
        correlations.sort(key=lambda x: x[1], reverse=True)
        max_corr = correlations[0][1]

        if max_corr < 0.1:
            pattern_type = "MCAR (ì™„ì „ ë¬´ì‘ìœ„ ê²°ì¸¡)"
            pattern_desc = "ê²°ì¸¡ê°’ì´ ë‹¤ë¥¸ ë³€ìˆ˜ë“¤ê³¼ ê±°ì˜ ìƒê´€ê´€ê³„ê°€ ì—†ìŠµë‹ˆë‹¤. ë¬´ì‘ìœ„ë¡œ ë°œìƒí•œ ê²ƒìœ¼ë¡œ ì¶”ì •ë©ë‹ˆë‹¤."
        elif max_corr < 0.3:
            pattern_type = "MAR ê°€ëŠ¥ì„± (ë¬´ì‘ìœ„ ê²°ì¸¡)"
            pattern_desc = "ê²°ì¸¡ê°’ì´ ë‹¤ë¥¸ ë³€ìˆ˜ë“¤ê³¼ ì•½í•œ ìƒê´€ê´€ê³„ë¥¼ ë³´ì…ë‹ˆë‹¤. ê´€ì¸¡ëœ ë‹¤ë¥¸ ë³€ìˆ˜ì— ì˜ì¡´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        else:
            pattern_type = "MNAR ê°€ëŠ¥ì„± (ë¹„ë¬´ì‘ìœ„ ê²°ì¸¡)"
            pattern_desc = "ê²°ì¸¡ê°’ì´ ë‹¤ë¥¸ ë³€ìˆ˜ë“¤ê³¼ ìƒë‹¹í•œ ìƒê´€ê´€ê³„ë¥¼ ë³´ì…ë‹ˆë‹¤. ê²°ì¸¡ ìì²´ê°€ íŠ¹ì • íŒ¨í„´ì„ ë”°ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤."

        lines.append(f"- **ì¶”ì • ìœ í˜•**: {pattern_type}")
        lines.append(f"- **ì„¤ëª…**: {pattern_desc}")
        lines.append(f"")
        lines.append(f"### ê´€ë ¨ ì»¬ëŸ¼ê³¼ì˜ ìƒê´€ê´€ê³„")
        for col_name, corr_val in correlations[:3]:
            lines.append(f"- {col_name}: {corr_val:.3f}")
    else:
        lines.append("- ìƒê´€ê´€ê³„ ë¶„ì„ì„ ìœ„í•œ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì´ ë¶€ì¡±í•©ë‹ˆë‹¤.")

    # ê²°ì¸¡ê°’ì´ ìˆëŠ” í–‰ì˜ íŠ¹ì„±
    missing_rows = df[missing_mask]
    non_missing_rows = df[~missing_mask]

    if len(numeric_cols) > 0:
        lines.append(f"")
        lines.append(f"### ê²°ì¸¡/ë¹„ê²°ì¸¡ ê·¸ë£¹ ë¹„êµ (ìˆ˜ì¹˜í˜• ì»¬ëŸ¼)")
        for other_col in numeric_cols[:3]:
            missing_mean = missing_rows[other_col].mean()
            non_missing_mean = non_missing_rows[other_col].mean()
            if not np.isnan(missing_mean) and not np.isnan(non_missing_mean):
                diff_pct = ((missing_mean - non_missing_mean) / non_missing_mean * 100) if non_missing_mean != 0 else 0
                lines.append(f"- {other_col}: ê²°ì¸¡ ê·¸ë£¹ í‰ê· ={missing_mean:.2f}, ë¹„ê²°ì¸¡ ê·¸ë£¹ í‰ê· ={non_missing_mean:.2f} (ì°¨ì´: {diff_pct:+.1f}%)")

    return "\n".join(lines)


def get_column_correlation_with_target(df: pd.DataFrame, target_column: str, **kwargs) -> str:
    """
    íŠ¹ì • íƒ€ê²Ÿ ì»¬ëŸ¼ê³¼ ë‹¤ë¥¸ ëª¨ë“  ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ë“¤ ê°„ì˜ ìƒê´€ê´€ê³„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.

    Parameters:
        df (pd.DataFrame): ë¶„ì„í•  DataFrame
        target_column (str): íƒ€ê²Ÿ ì»¬ëŸ¼ëª…

    Returns:
        str: ìƒê´€ê´€ê³„ ë¶„ì„ ê²°ê³¼ ë¬¸ìì—´
    """
    if target_column not in df.columns:
        return f"'{target_column}' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    if not pd.api.types.is_numeric_dtype(df[target_column]):
        return f"'{target_column}' ì»¬ëŸ¼ì€ ìˆ˜ì¹˜í˜•ì´ ì•„ë‹™ë‹ˆë‹¤. ìƒê´€ê´€ê³„ ë¶„ì„ì—ëŠ” ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤."

    numeric_df = df.select_dtypes(include=[np.number])
    if len(numeric_df.columns) < 2:
        return "ìƒê´€ê´€ê³„ ë¶„ì„ì—ëŠ” ìµœì†Œ 2ê°œì˜ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤."

    correlations = []
    for col in numeric_df.columns:
        if col != target_column:
            corr = numeric_df[target_column].corr(numeric_df[col])
            if not np.isnan(corr):
                correlations.append((col, corr))

    if not correlations:
        return "ìƒê´€ê´€ê³„ë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆëŠ” ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤."

    # ìƒê´€ê³„ìˆ˜ ì ˆëŒ€ê°’ ê¸°ì¤€ ì •ë ¬
    correlations.sort(key=lambda x: abs(x[1]), reverse=True)

    lines = [
        f"## '{target_column}' ì»¬ëŸ¼ê³¼ì˜ ìƒê´€ê´€ê³„ ë¶„ì„",
        f"",
        f"### ìƒê´€ê³„ìˆ˜ ìˆœìœ„ (ì ˆëŒ€ê°’ ê¸°ì¤€)"
    ]

    for idx, (col, corr) in enumerate(correlations, 1):
        # ìƒê´€ê´€ê³„ ê°•ë„ í•´ì„
        abs_corr = abs(corr)
        if abs_corr >= 0.7:
            strength = "ğŸ”´ ê°•í•¨"
        elif abs_corr >= 0.4:
            strength = "ğŸŸ¡ ì¤‘ê°„"
        elif abs_corr >= 0.2:
            strength = "ğŸŸ¢ ì•½í•¨"
        else:
            strength = "âšª ë§¤ìš° ì•½í•¨"

        direction = "ì–‘ì˜ ìƒê´€" if corr > 0 else "ìŒì˜ ìƒê´€"
        lines.append(f"{idx}. {col}: {corr:+.3f} ({strength}, {direction})")

    # ìš”ì•½
    strong_corrs = [c for c in correlations if abs(c[1]) >= 0.4]
    if strong_corrs:
        lines.append(f"")
        lines.append(f"### ìš”ì•½")
        lines.append(f"- ì¤‘ê°„ ì´ìƒ ìƒê´€ê´€ê³„: {len(strong_corrs)}ê°œ ì»¬ëŸ¼")
        lines.append(f"- ê°€ì¥ ê°•í•œ ìƒê´€: {correlations[0][0]} ({correlations[0][1]:+.3f})")

    return "\n".join(lines)


def detect_data_types(df: pd.DataFrame, **kwargs) -> str:
    """
    ì»¬ëŸ¼ë³„ ì‹¤ì œ ë°ì´í„° íƒ€ì…ì„ ì¶”ë¡ í•©ë‹ˆë‹¤.

    Parameters:
        df (pd.DataFrame): ë¶„ì„í•  DataFrame

    Returns:
        str: ë°ì´í„° íƒ€ì… ì¶”ë¡  ê²°ê³¼ ë¬¸ìì—´
    """
    if df.empty:
        return "ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤ (ë¹ˆ DataFrame)."

    lines = [
        f"## ì»¬ëŸ¼ë³„ ë°ì´í„° íƒ€ì… ë¶„ì„",
        f"",
        f"| ì»¬ëŸ¼ëª… | pandas íƒ€ì… | ì¶”ë¡  íƒ€ì… | ë¹„ê³  |",
        f"|--------|-------------|-----------|------|"
    ]

    for col in df.columns:
        pandas_dtype = str(df[col].dtype)
        sample = df[col].dropna()

        if len(sample) == 0:
            inferred_type = "ì•Œ ìˆ˜ ì—†ìŒ"
            note = "ëª¨ë“  ê°’ì´ ê²°ì¸¡"
        elif pd.api.types.is_numeric_dtype(df[col]):
            # ì •ìˆ˜/ì‹¤ìˆ˜ êµ¬ë¶„
            if pd.api.types.is_integer_dtype(df[col]):
                unique_ratio = df[col].nunique() / len(sample)
                if unique_ratio < 0.05:
                    inferred_type = "ë²”ì£¼í˜• (ì½”ë“œ)"
                    note = f"ê³ ìœ ê°’ {df[col].nunique()}ê°œ"
                else:
                    inferred_type = "ì •ìˆ˜"
                    note = ""
            else:
                inferred_type = "ì‹¤ìˆ˜"
                note = ""
        elif pd.api.types.is_datetime64_any_dtype(df[col]):
            inferred_type = "ë‚ ì§œ/ì‹œê°„"
            note = ""
        else:
            # ë¬¸ìì—´ íƒ€ì… ì„¸ë¶€ ë¶„ì„
            sample_vals = sample.astype(str).head(100)
            inferred_type = None
            note = ""

            # ë‚ ì§œ í˜•ì‹ ì²´í¬
            try:
                pd.to_datetime(sample_vals, errors='raise')
                inferred_type = "ë‚ ì§œ (ë¬¸ìì—´)"
                note = "datetime ë³€í™˜ ê°€ëŠ¥"
            except (ValueError, TypeError):
                pass

            # ìˆ«ì í˜•ì‹ ì²´í¬
            if inferred_type is None:
                try:
                    pd.to_numeric(sample_vals, errors='raise')
                    inferred_type = "ìˆ«ì (ë¬¸ìì—´)"
                    note = "numeric ë³€í™˜ ê°€ëŠ¥"
                except (ValueError, TypeError):
                    pass

            # ì¼ë°˜ ë²”ì£¼í˜•
            if inferred_type is None:
                unique_count = df[col].nunique()
                if unique_count <= 20:
                    inferred_type = "ë²”ì£¼í˜•"
                    note = f"ê³ ìœ ê°’ {unique_count}ê°œ"
                elif unique_count <= len(df) * 0.5:
                    inferred_type = "ë²”ì£¼í˜• (ë‹¤ìˆ˜)"
                    note = f"ê³ ìœ ê°’ {unique_count}ê°œ"
                else:
                    inferred_type = "í…ìŠ¤íŠ¸/ID"
                    note = "ê³ ìœ ê°’ ë¹„ìœ¨ ë†’ìŒ"

        lines.append(f"| {col} | {pandas_dtype} | {inferred_type} | {note} |")

    return "\n".join(lines)


def get_temporal_pattern(df: pd.DataFrame, column: str, **kwargs) -> str:
    """
    ì‹œê°„/ë‚ ì§œ ê´€ë ¨ ì»¬ëŸ¼ì˜ íŒ¨í„´ì„ ë¶„ì„í•©ë‹ˆë‹¤.

    Parameters:
        df (pd.DataFrame): ë¶„ì„í•  DataFrame
        column (str): ì‹œê°„/ë‚ ì§œ ì»¬ëŸ¼ëª…

    Returns:
        str: ì‹œê°„ íŒ¨í„´ ë¶„ì„ ê²°ê³¼ ë¬¸ìì—´
    """
    if column not in df.columns:
        return f"'{column}' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    try:
        date_col = pd.to_datetime(df[column], errors='coerce')
        valid_dates = date_col.dropna()

        if len(valid_dates) == 0:
            return f"'{column}' ì»¬ëŸ¼ì— ìœ íš¨í•œ ë‚ ì§œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."

        lines = [
            f"## '{column}' ì»¬ëŸ¼ ì‹œê°„ íŒ¨í„´ ë¶„ì„",
            f"",
            f"### ê¸°ë³¸ ì •ë³´",
            f"- ìœ íš¨ ë‚ ì§œ ìˆ˜: {len(valid_dates):,}ê°œ",
            f"- ê¸°ê°„: {valid_dates.min().strftime('%Y-%m-%d')} ~ {valid_dates.max().strftime('%Y-%m-%d')}"
        ]

        # ì—°ë„ë³„ ë¶„í¬
        if valid_dates.dt.year.nunique() > 1:
            year_dist = valid_dates.dt.year.value_counts().sort_index()
            lines.append(f"")
            lines.append(f"### ì—°ë„ë³„ ë¶„í¬")
            for year, count in year_dist.items():
                pct = count / len(valid_dates) * 100
                lines.append(f"- {year}ë…„: {count:,}ê°œ ({pct:.1f}%)")

        # ì›”ë³„ ë¶„í¬
        month_dist = valid_dates.dt.month.value_counts().sort_index()
        lines.append(f"")
        lines.append(f"### ì›”ë³„ ë¶„í¬")
        month_names = ['1ì›”', '2ì›”', '3ì›”', '4ì›”', '5ì›”', '6ì›”',
                       '7ì›”', '8ì›”', '9ì›”', '10ì›”', '11ì›”', '12ì›”']
        for month, count in month_dist.items():
            pct = count / len(valid_dates) * 100
            lines.append(f"- {month_names[month-1]}: {count:,}ê°œ ({pct:.1f}%)")

        # ìš”ì¼ë³„ ë¶„í¬
        day_dist = valid_dates.dt.dayofweek.value_counts().sort_index()
        lines.append(f"")
        lines.append(f"### ìš”ì¼ë³„ ë¶„í¬")
        day_names = ['ì›”ìš”ì¼', 'í™”ìš”ì¼', 'ìˆ˜ìš”ì¼', 'ëª©ìš”ì¼', 'ê¸ˆìš”ì¼', 'í† ìš”ì¼', 'ì¼ìš”ì¼']
        for day, count in day_dist.items():
            pct = count / len(valid_dates) * 100
            lines.append(f"- {day_names[day]}: {count:,}ê°œ ({pct:.1f}%)")

        # ì‹œê°„ëŒ€ë³„ ë¶„í¬ (ì‹œê°„ ì •ë³´ê°€ ìˆëŠ” ê²½ìš°)
        if valid_dates.dt.hour.nunique() > 1:
            hour_dist = valid_dates.dt.hour.value_counts().sort_index()
            lines.append(f"")
            lines.append(f"### ì‹œê°„ëŒ€ë³„ ë¶„í¬ (ìƒìœ„ 5ê°œ)")
            for hour, count in hour_dist.head(5).items():
                pct = count / len(valid_dates) * 100
                lines.append(f"- {hour}ì‹œ: {count:,}ê°œ ({pct:.1f}%)")

        return "\n".join(lines)

    except Exception as e:
        return f"ì‹œê°„ íŒ¨í„´ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"


def summarize_categorical_distribution(df: pd.DataFrame, column: str, **kwargs) -> str:
    """
    ë²”ì£¼í˜• ì»¬ëŸ¼ì˜ ë¶„í¬ë¥¼ ìƒì„¸í•˜ê²Œ ìš”ì•½í•©ë‹ˆë‹¤.

    Parameters:
        df (pd.DataFrame): ë¶„ì„í•  DataFrame
        column (str): ë²”ì£¼í˜• ì»¬ëŸ¼ëª…

    Returns:
        str: ë²”ì£¼í˜• ë¶„í¬ ìš”ì•½ ë¬¸ìì—´
    """
    if column not in df.columns:
        return f"'{column}' ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    value_counts = df[column].value_counts()
    total = len(df)
    unique_count = len(value_counts)
    missing_count = df[column].isnull().sum()

    lines = [
        f"## '{column}' ì»¬ëŸ¼ ë²”ì£¼í˜• ë¶„í¬ ë¶„ì„",
        f"",
        f"### ê¸°ë³¸ í†µê³„",
        f"- ì „ì²´ í–‰ ìˆ˜: {total:,}",
        f"- ê³ ìœ  ì¹´í…Œê³ ë¦¬ ìˆ˜: {unique_count}",
        f"- ê²°ì¸¡ê°’: {missing_count:,}ê°œ ({missing_count/total*100:.1f}%)"
    ]

    # ì§‘ì¤‘ë„ ë¶„ì„
    if unique_count > 0:
        top1_pct = value_counts.iloc[0] / (total - missing_count) * 100 if (total - missing_count) > 0 else 0
        top3_pct = value_counts.head(3).sum() / (total - missing_count) * 100 if (total - missing_count) > 0 else 0

        lines.append(f"")
        lines.append(f"### ì§‘ì¤‘ë„ ë¶„ì„")
        lines.append(f"- ìµœë¹ˆê°’ ë¹„ìœ¨: {top1_pct:.1f}% ({value_counts.index[0]})")
        lines.append(f"- ìƒìœ„ 3ê°œ ë¹„ìœ¨: {top3_pct:.1f}%")

        # í¸í–¥ì„± íŒë‹¨
        if top1_pct > 80:
            bias = "ğŸ”´ ë§¤ìš° í¸í–¥ë¨ (ë‹¨ì¼ ê°’ì´ 80% ì´ìƒ)"
        elif top1_pct > 50:
            bias = "ğŸŸ¡ í¸í–¥ë¨ (ë‹¨ì¼ ê°’ì´ 50% ì´ìƒ)"
        elif top3_pct > 80:
            bias = "ğŸŸ¡ ì•½ê°„ í¸í–¥ë¨ (ìƒìœ„ 3ê°œê°€ 80% ì´ìƒ)"
        else:
            bias = "ğŸŸ¢ ê· í˜•ì  ë¶„í¬"
        lines.append(f"- í¸í–¥ì„±: {bias}")

    # í¬ê·€ ì¹´í…Œê³ ë¦¬ ë¶„ì„
    rare_threshold = total * 0.01  # 1% ë¯¸ë§Œ
    rare_categories = value_counts[value_counts < rare_threshold]
    if len(rare_categories) > 0:
        lines.append(f"")
        lines.append(f"### í¬ê·€ ì¹´í…Œê³ ë¦¬ (1% ë¯¸ë§Œ)")
        lines.append(f"- í¬ê·€ ì¹´í…Œê³ ë¦¬ ìˆ˜: {len(rare_categories)}ê°œ")
        lines.append(f"- í¬ê·€ ì¹´í…Œê³ ë¦¬ í•©ê³„: {rare_categories.sum():,}ê°œ ({rare_categories.sum()/total*100:.2f}%)")
        if len(rare_categories) <= 10:
            for cat, count in rare_categories.items():
                lines.append(f"  - {cat}: {count}ê°œ")

    # ìƒìœ„ ì¹´í…Œê³ ë¦¬
    lines.append(f"")
    lines.append(f"### ìƒìœ„ ì¹´í…Œê³ ë¦¬ (ìµœëŒ€ 10ê°œ)")
    for idx, (cat, count) in enumerate(value_counts.head(10).items(), 1):
        pct = count / (total - missing_count) * 100 if (total - missing_count) > 0 else 0
        lines.append(f"{idx}. {cat}: {count:,}ê°œ ({pct:.1f}%)")

    return "\n".join(lines)


# ============================================================================
# Tool Dispatcher (T026)
# ============================================================================

# ë„êµ¬ ì´ë¦„ê³¼ í•¸ë“¤ëŸ¬ í•¨ìˆ˜ ë§¤í•‘
TOOL_HANDLERS = {
    "get_dataframe_info": get_dataframe_info,
    "get_column_statistics": get_column_statistics,
    "get_missing_values": get_missing_values,
    "get_value_counts": get_value_counts,
    "filter_dataframe": filter_dataframe,
    "sort_dataframe": sort_dataframe,
    "get_correlation": get_correlation,
    "group_by_aggregate": group_by_aggregate,
    "get_unique_values": get_unique_values,
    "get_date_range": get_date_range,
    "get_outliers": get_outliers,
    "get_sample_rows": get_sample_rows,
    "calculate_percentile": calculate_percentile,
    "get_geo_bounds": get_geo_bounds,
    "cross_tabulation": cross_tabulation,
    # v1.1.2: 5ê°œ ì¶”ê°€ ë„êµ¬
    "analyze_missing_pattern": analyze_missing_pattern,
    "get_column_correlation_with_target": get_column_correlation_with_target,
    "detect_data_types": detect_data_types,
    "get_temporal_pattern": get_temporal_pattern,
    "summarize_categorical_distribution": summarize_categorical_distribution,
}


def execute_tool(tool_name: str, tool_input: dict, df: pd.DataFrame) -> str:
    """
    ë„êµ¬ë¥¼ ì‹¤í–‰í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.

    Parameters:
        tool_name (str): ì‹¤í–‰í•  ë„êµ¬ ì´ë¦„
        tool_input (dict): ë„êµ¬ ì…ë ¥ íŒŒë¼ë¯¸í„°
        df (pd.DataFrame): ë¶„ì„í•  DataFrame

    Returns:
        str: ë„êµ¬ ì‹¤í–‰ ê²°ê³¼ ë¬¸ìì—´
    """
    if tool_name not in TOOL_HANDLERS:
        return f"ì•Œ ìˆ˜ ì—†ëŠ” ë„êµ¬ì…ë‹ˆë‹¤: {tool_name}"

    try:
        handler = TOOL_HANDLERS[tool_name]
        result = handler(df, **tool_input)
        return result
    except Exception as e:
        return f"ë„êµ¬ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}"
