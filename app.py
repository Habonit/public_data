"""
Daegu Public Data Visualization - Streamlit Application

An educational tool for exploring and analyzing public datasets from Daegu.
Provides individual dataset exploration, cross-dataset spatial analysis, and
educational content to help data analysis learners discover insights independently.
"""
import io
import streamlit as st
import pandas as pd
import plotly.express as px
from streamlit_folium import st_folium
from utils.loader import load_dataset, load_dataset_from_session, get_dataset_info, read_csv_safe, read_uploaded_csv
from utils.geo import detect_lat_lng_columns
from utils.visualizer import (
    plot_numeric_distribution,
    plot_categorical_distribution,
    plot_boxplot,
    plot_kde,
    plot_scatter,
    plot_with_options,
    check_missing_ratio,
    create_folium_map,
    create_overlay_map
)
from utils.chatbot import (
    SYSTEM_PROMPT,
    create_data_context,
    create_chat_response,
    handle_chat_error,
    validate_api_key
)
from anthropic import Anthropic

# ë°ì´í„°ì…‹ ë§¤í•‘ ìƒìˆ˜
DATASET_MAPPING = {
    'cctv': {
        'display_name': 'CCTV',
        'tab_icon': 'ğŸ¥',
        'expected_file': 'ëŒ€êµ¬ CCTV ì •ë³´.csv',
        'color': 'red'
    },
    'lights': {
        'display_name': 'ë³´ì•ˆë“±',
        'tab_icon': 'ğŸ’¡',
        'expected_file': 'ëŒ€êµ¬ ë³´ì•ˆë“± ì •ë³´.csv',
        'color': 'blue'
    },
    'zones': {
        'display_name': 'ì–´ë¦°ì´ ë³´í˜¸êµ¬ì—­',
        'tab_icon': 'ğŸ«',
        'expected_file': 'ëŒ€êµ¬ ì–´ë¦°ì´ ë³´í˜¸ êµ¬ì—­ ì •ë³´.csv',
        'color': 'green'
    },
    'parking': {
        'display_name': 'ì£¼ì°¨ì¥',
        'tab_icon': 'ğŸ…¿ï¸',
        'expected_file': 'ëŒ€êµ¬ ì£¼ì°¨ì¥ ì •ë³´.csv',
        'color': 'purple'
    },
    'accident': {
        'display_name': 'ì‚¬ê³ ',
        'tab_icon': 'ğŸš—',
        'expected_file': 'countrywide_accident.csv',
        'color': 'orange'
    },
    'train': {
        'display_name': 'í›ˆë ¨ ë°ì´í„°',
        'tab_icon': 'ğŸ“Š',
        'expected_file': 'train.csv',
        'color': 'darkred'
    },
    'test': {
        'display_name': 'í…ŒìŠ¤íŠ¸ ë°ì´í„°',
        'tab_icon': 'ğŸ“‹',
        'expected_file': 'test.csv',
        'color': 'darkblue'
    }
}

# AI ëª¨ë¸ ì˜µì…˜
AI_MODEL_OPTIONS = [
    {'id': 'claude-sonnet-4-20250514', 'name': 'Claude Sonnet 4', 'description': 'ë¹ ë¥¸ ì‘ë‹µ, ë¹„ìš© íš¨ìœ¨ì  (ê¶Œì¥)'},
    {'id': 'claude-opus-4-20250514', 'name': 'Claude Opus 4', 'description': 'ë³µì¡í•œ ë¶„ì„ì— ì í•©'},
    {'id': 'claude-3-5-haiku-20241022', 'name': 'Claude 3.5 Haiku', 'description': 'ê°„ë‹¨í•œ ì§ˆë¬¸ì— ìµœì '}
]


def init_session_state():
    """
    session_state ì´ˆê¸°í™”.
    ì•± ì‹œì‘ ì‹œ í•œ ë²ˆ í˜¸ì¶œ.
    """
    if 'initialized' in st.session_state:
        return

    # ë°ì´í„°ì…‹ ì €ì¥ì†Œ
    if 'datasets' not in st.session_state:
        st.session_state.datasets = {}

    # ì—…ë¡œë“œ ìƒíƒœ
    if 'upload_status' not in st.session_state:
        st.session_state.upload_status = {
            key: False for key in DATASET_MAPPING.keys()
        }

    # ì±—ë´‡ ì„¸ì…˜
    if 'chatbot' not in st.session_state:
        st.session_state.chatbot = {
            'api_key': '',
            'model': 'claude-sonnet-4-20250514',
            'selected_dataset': None,
            'messages': [],
            'tokens': {'total': 0, 'input': 0, 'output': 0}
        }

    st.session_state.initialized = True

# Page configuration
st.set_page_config(
    page_title="ëŒ€êµ¬ ê³µê³µë°ì´í„° ì‹œê°í™”",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)


def render_dataset_tab(dataset_name: str, dataset_display_name: str):
    """
    Render a complete tab for exploring an individual dataset.

    Parameters:
        dataset_name (str): Internal dataset name for load_dataset()
        dataset_display_name (str): Display name for UI
    """
    st.header(f"{dataset_display_name} ë°ì´í„°ì…‹")

    # Check if dataset is uploaded (T020, T021)
    if not st.session_state.upload_status.get(dataset_name, False):
        st.info(f"ğŸ“¤ **{dataset_display_name}** ë°ì´í„°ë¥¼ ë¨¼ì € ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
        st.markdown("**í”„ë¡œì íŠ¸ ê°œìš”** íƒ­ì—ì„œ CSV íŒŒì¼ì„ ì—…ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        return

    # Load dataset from session_state (T022)
    df = load_dataset_from_session(dataset_name)
    if df is None:
        st.warning(f"âš ï¸ {dataset_display_name} ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
        return

    # Get dataset info
    info = get_dataset_info(df)

    # Display basic statistics
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("ì „ì²´ í–‰ ìˆ˜", f"{info['row_count']:,}")
    with col2:
        st.metric("ì „ì²´ ì»¬ëŸ¼ ìˆ˜", info['column_count'])
    with col3:
        missing_pct = sum(info['missing_ratios'].values()) / len(info['missing_ratios']) * 100 if info['missing_ratios'] else 0
        st.metric("í‰ê·  ê²°ì¸¡ê°’ %", f"{missing_pct:.1f}%")

    # Data Preview
    with st.expander("ğŸ“‹ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 10ê°œ í–‰)", expanded=False):
        st.dataframe(df.head(10), use_container_width=True)

    # Column Information
    with st.expander("ğŸ“Š ì»¬ëŸ¼ ì •ë³´", expanded=False):
        col_info_df = []
        for col in df.columns:
            col_info_df.append({
                'ì»¬ëŸ¼': col,
                'íƒ€ì…': info['dtypes'][col],
                'ê²°ì¸¡ê°’ %': f"{info['missing_ratios'][col] * 100:.1f}%"
            })
        st.dataframe(col_info_df, use_container_width=True)

    # Descriptive Statistics for Numeric Columns
    if not info['numeric_summary'].empty:
        with st.expander("ğŸ“ˆ ìˆ«ì ì»¬ëŸ¼ í†µê³„", expanded=False):
            st.dataframe(info['numeric_summary'], use_container_width=True)

    # Visualizations
    st.subheader("ì‹œê°í™”")

    # Detect coordinates
    lat_col, lng_col = detect_lat_lng_columns(df)

    # Map Visualization
    if lat_col and lng_col:
        st.markdown("### ğŸ—ºï¸ ì§€ë¦¬ì  ë¶„í¬")
        st.info(f"ê°ì§€ëœ ì¢Œí‘œ: **{lat_col}** (ìœ„ë„), **{lng_col}** (ê²½ë„)")

        # Get columns for popup (exclude coordinate columns, limit to first 3 non-numeric)
        popup_candidates = [col for col in df.columns if col not in [lat_col, lng_col]]
        popup_cols = popup_candidates[:3]  # Show first 3 columns in popup

        # Create map
        map_obj = create_folium_map(
            df, lat_col, lng_col,
            popup_cols=popup_cols,
            color='blue',
            name=dataset_display_name
        )

        # Display map
        st_folium(map_obj, width=700, height=500)
    else:
        st.info("â„¹ï¸ ì§€ë¦¬ ì¢Œí‘œê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì´ ë°ì´í„°ì…‹ì—ëŠ” ì§€ë„ ì‹œê°í™”ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # Numeric Distributions (T029-T033: ì°¨íŠ¸ ìœ í˜• ì„ íƒ, ê²°ì¸¡ì¹˜ ê²½ê³ )
    numeric_cols = df.select_dtypes(include=['number']).columns.tolist()
    if numeric_cols:
        st.markdown("### ğŸ“Š ìˆ«ì ì»¬ëŸ¼ ë¶„í¬")

        # T029: Chart type selection
        chart_type = st.selectbox(
            "ì°¨íŠ¸ ìœ í˜• ì„ íƒ:",
            options=['íˆìŠ¤í† ê·¸ë¨', 'ë°•ìŠ¤í”Œë¡¯', 'KDE', 'ì‚°ì ë„'],
            key=f"{dataset_name}_chart_type"
        )

        chart_type_map = {
            'íˆìŠ¤í† ê·¸ë¨': 'histogram',
            'ë°•ìŠ¤í”Œë¡¯': 'boxplot',
            'KDE': 'kde',
            'ì‚°ì ë„': 'scatter'
        }

        # T030: For scatter plot, show X/Y column selection
        if chart_type == 'ì‚°ì ë„':
            col1, col2 = st.columns(2)
            with col1:
                x_col = st.selectbox(
                    "Xì¶• ì»¬ëŸ¼:",
                    options=numeric_cols,
                    key=f"{dataset_name}_scatter_x"
                )
            with col2:
                y_col = st.selectbox(
                    "Yì¶• ì»¬ëŸ¼:",
                    options=[c for c in numeric_cols if c != x_col] if len(numeric_cols) > 1 else numeric_cols,
                    key=f"{dataset_name}_scatter_y"
                )

            # T033: Missing value warning for scatter
            x_warning, x_ratio = check_missing_ratio(df, x_col)
            y_warning, y_ratio = check_missing_ratio(df, y_col)
            if x_warning:
                st.warning(f"âš ï¸ {x_col} ì»¬ëŸ¼ì˜ ê²°ì¸¡ê°’ì´ {x_ratio*100:.1f}%ì…ë‹ˆë‹¤. ê²°ê³¼ê°€ ì™œê³¡ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            if y_warning:
                st.warning(f"âš ï¸ {y_col} ì»¬ëŸ¼ì˜ ê²°ì¸¡ê°’ì´ {y_ratio*100:.1f}%ì…ë‹ˆë‹¤. ê²°ê³¼ê°€ ì™œê³¡ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

            # T031: Render scatter plot
            fig = plot_scatter(df, x_col, y_col)
            st.plotly_chart(fig, use_container_width=True)
        else:
            # Single column selection for other chart types
            selected_numeric_col = st.selectbox(
                "ì‹œê°í™”í•  ìˆ«ì ì»¬ëŸ¼ ì„ íƒ:",
                options=numeric_cols,
                key=f"{dataset_name}_numeric_select"
            )

            if selected_numeric_col:
                # T033: Missing value warning
                is_high_missing, missing_ratio = check_missing_ratio(df, selected_numeric_col)
                if is_high_missing:
                    st.warning(f"âš ï¸ {selected_numeric_col} ì»¬ëŸ¼ì˜ ê²°ì¸¡ê°’ì´ {missing_ratio*100:.1f}%ì…ë‹ˆë‹¤. ê²°ê³¼ê°€ ì™œê³¡ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

                # T031: Render selected chart type
                fig = plot_with_options(df, selected_numeric_col, chart_type_map[chart_type])
                st.plotly_chart(fig, use_container_width=True)
    else:
        st.info("â„¹ï¸ ì´ ë°ì´í„°ì…‹ì—ëŠ” ìˆ«ìí˜• ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    # Categorical Distributions
    categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
    if categorical_cols:
        st.markdown("### ğŸ“Š ë²”ì£¼í˜• ì»¬ëŸ¼ ë¶„í¬")

        # Let user select which categorical column to visualize
        selected_cat_col = st.selectbox(
            "ì‹œê°í™”í•  ë²”ì£¼í˜• ì»¬ëŸ¼ ì„ íƒ:",
            options=categorical_cols,
            key=f"{dataset_name}_cat_select"
        )

        if selected_cat_col:
            fig = plot_categorical_distribution(df, selected_cat_col)
            st.plotly_chart(fig, use_container_width=True)
    else:
        st.info("â„¹ï¸ ì´ ë°ì´í„°ì…‹ì—ëŠ” ë²”ì£¼í˜• ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")


def render_overview_tab():
    """
    Render the project overview tab with upload functionality. (T016-T019)
    """
    st.header("ğŸ“– í”„ë¡œì íŠ¸ ê°œìš”")

    # Project Introduction
    st.markdown("""
    ## ëŒ€êµ¬ ê³µê³µë°ì´í„° ì‹œê°í™” í”„ë¡œì íŠ¸

    ì´ í”„ë¡œì íŠ¸ëŠ” ëŒ€êµ¬ì‹œì˜ ë‹¤ì–‘í•œ ê³µê³µ ë°ì´í„°ë¥¼ íƒìƒ‰í•˜ê³  ë¶„ì„í•  ìˆ˜ ìˆëŠ” ëŒ€í™”í˜• ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜ì…ë‹ˆë‹¤.
    ë°ì´í„° ë¶„ì„ì„ í•™ìŠµí•˜ëŠ” ì‚¬ìš©ìë“¤ì´ ì‹¤ì œ ê³µê³µ ë°ì´í„°ë¥¼ í†µí•´ ì¸ì‚¬ì´íŠ¸ë¥¼ ë°œê²¬í•˜ê³ 
    ë°ì´í„° ì‹œê°í™” ê¸°ìˆ ì„ ìµí ìˆ˜ ìˆë„ë¡ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.
    """)

    # Data Upload Section (T017-T019)
    st.subheader("ğŸ“¤ ë°ì´í„° ì—…ë¡œë“œ")
    st.markdown("ê° ë°ì´í„°ì…‹ì— í•´ë‹¹í•˜ëŠ” CSV íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.")

    # Display upload status
    uploaded_count = sum(st.session_state.upload_status.values())
    st.info(f"ì—…ë¡œë“œ í˜„í™©: {uploaded_count} / {len(DATASET_MAPPING)} ë°ì´í„°ì…‹")

    # Create upload widgets for each dataset
    for dataset_key, dataset_info in DATASET_MAPPING.items():
        with st.expander(
            f"{dataset_info['tab_icon']} {dataset_info['display_name']} "
            f"({'âœ… ì—…ë¡œë“œë¨' if st.session_state.upload_status.get(dataset_key) else 'â³ ëŒ€ê¸°ì¤‘'})"
        ):
            st.markdown(f"**ì˜ˆìƒ íŒŒì¼ëª…**: `{dataset_info['expected_file']}`")

            uploaded_file = st.file_uploader(
                f"{dataset_info['display_name']} CSV íŒŒì¼ ì„ íƒ",
                type=['csv'],
                key=f"upload_{dataset_key}"
            )

            if uploaded_file is not None:
                try:
                    df = read_uploaded_csv(uploaded_file)
                    # Store in session_state (T018)
                    st.session_state.datasets[dataset_key] = df
                    st.session_state.upload_status[dataset_key] = True

                    # Display upload info (T019)
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.metric("íŒŒì¼ëª…", uploaded_file.name)
                    with col2:
                        file_size_kb = uploaded_file.size / 1024
                        if file_size_kb > 1024:
                            st.metric("íŒŒì¼ í¬ê¸°", f"{file_size_kb/1024:.2f} MB")
                        else:
                            st.metric("íŒŒì¼ í¬ê¸°", f"{file_size_kb:.2f} KB")
                    with col3:
                        st.metric("í–‰ x ì»¬ëŸ¼", f"{len(df):,} x {len(df.columns)}")

                    st.success(f"âœ… {dataset_info['display_name']} ë°ì´í„° ì—…ë¡œë“œ ì™„ë£Œ!")

                    # Show preview
                    with st.expander("ğŸ“‹ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°", expanded=False):
                        st.dataframe(df.head(5), use_container_width=True)

                except Exception as e:
                    st.error(f"âŒ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜: {str(e)}")

    st.markdown("---")

    # Key Features
    st.subheader("ğŸ¯ ì£¼ìš” ê¸°ëŠ¥")
    feature_col1, feature_col2 = st.columns(2)

    with feature_col1:
        st.markdown("""
        **ê°œë³„ ë°ì´í„°ì…‹ íƒìƒ‰**
        - 7ê°œì˜ ê³µê³µ ë°ì´í„°ì…‹ ì§€ì›
        - ê¸°ë³¸ í†µê³„ ë° ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°
        - ëŒ€í™”í˜• ì°¨íŠ¸ ë° ê·¸ë˜í”„
        - ì§€ë¦¬ì  ë¶„í¬ ì§€ë„ ì‹œê°í™”
        """)

        st.markdown("""
        **êµì°¨ ë°ì´í„° ë¶„ì„**
        - ì—¬ëŸ¬ ë°ì´í„°ì…‹ ë™ì‹œ ì‹œê°í™”
        - ê³µê°„ì  ê´€ê³„ ë¶„ì„
        """)

    with feature_col2:
        st.markdown("""
        **ì‚¬ìš©ì ì¹œí™”ì  ì¸í„°í˜ì´ìŠ¤**
        - ì§ê´€ì ì¸ íƒ­ ê¸°ë°˜ ë„¤ë¹„ê²Œì´ì…˜
        - ë°˜ì‘í˜• ë ˆì´ì•„ì›ƒ
        - ì‹¤ì‹œê°„ ë°ì´í„° í•„í„°ë§
        - ë‹¤ì–‘í•œ ì‹œê°í™” ì˜µì…˜
        """)

        st.markdown("""
        **AI ë°ì´í„° ë¶„ì„**
        - ìì—°ì–´ ì§ˆì˜ì‘ë‹µ
        - ë°ì´í„° ì¸ì‚¬ì´íŠ¸ ì œì•ˆ
        """)

    # How to Use
    st.subheader("ğŸ“š ì‚¬ìš© ë°©ë²•")

    st.markdown("""
    ### 1ï¸âƒ£ ë°ì´í„° ì—…ë¡œë“œ
    1. ìœ„ì˜ ê° ë°ì´í„°ì…‹ ì„¹ì…˜ì„ ì—´ì–´ CSV íŒŒì¼ì„ ì—…ë¡œë“œí•©ë‹ˆë‹¤
    2. ì—…ë¡œë“œëœ íŒŒì¼ ì •ë³´ì™€ ë¯¸ë¦¬ë³´ê¸°ë¥¼ í™•ì¸í•©ë‹ˆë‹¤

    ### 2ï¸âƒ£ ê°œë³„ ë°ì´í„°ì…‹ íƒìƒ‰
    1. í•´ë‹¹ ë°ì´í„°ì…‹ íƒ­ì„ ì„ íƒí•©ë‹ˆë‹¤
    2. ê¸°ë³¸ í†µê³„ì™€ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°ë¥¼ í™•ì¸í•©ë‹ˆë‹¤
    3. ìˆ«ìí˜•/ë²”ì£¼í˜• ì»¬ëŸ¼ì„ ì„ íƒí•˜ì—¬ ë¶„í¬ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤

    ### 3ï¸âƒ£ AI ë°ì´í„° ë¶„ì„
    1. ì‚¬ì´ë“œë°”ì—ì„œ API Keyë¥¼ ì…ë ¥í•©ë‹ˆë‹¤
    2. "ë°ì´í„° ì§ˆì˜ì‘ë‹µ" íƒ­ì—ì„œ ì§ˆë¬¸ì„ ì…ë ¥í•©ë‹ˆë‹¤
    """)

    # Technical Information
    st.subheader("ğŸ”§ ê¸°ìˆ  ìŠ¤íƒ")

    tech_col1, tech_col2, tech_col3 = st.columns(3)

    with tech_col1:
        st.markdown("""
        **í”„ë¡ íŠ¸ì—”ë“œ**
        - Streamlit
        - Plotly
        - Folium
        """)

    with tech_col2:
        st.markdown("""
        **ë°ì´í„° ì²˜ë¦¬**
        - Pandas
        - NumPy
        """)

    with tech_col3:
        st.markdown("""
        **AI**
        - Anthropic Claude
        - Python 3.10+
        """)


def render_cross_analysis_tab():
    """
    Render the cross-data analysis tab. (T023, T024 - simplified, no proximity analysis)
    """
    st.header("ğŸ”„ êµì°¨ ë°ì´í„° ë¶„ì„")
    st.markdown("""
    ì—¬ëŸ¬ ë°ì´í„°ì…‹ì„ ë™ì‹œì— ì§€ë„ ìœ„ì— í‘œì‹œí•˜ì—¬ ê³µê°„ì  ê´€ê³„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
    """)

    # Check if any datasets are uploaded
    uploaded_datasets = [
        key for key, uploaded in st.session_state.upload_status.items()
        if uploaded
    ]

    if not uploaded_datasets:
        st.info("ğŸ“¤ ë¨¼ì € **í”„ë¡œì íŠ¸ ê°œìš”** íƒ­ì—ì„œ ë°ì´í„°ì…‹ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
        return

    # Dataset selection
    st.subheader("ë°ì´í„°ì…‹ ì„ íƒ")

    available_options = {
        DATASET_MAPPING[key]['display_name']: key
        for key in uploaded_datasets
    }

    # Multi-select for datasets
    selected_names = st.multiselect(
        "ë¶„ì„í•  ë°ì´í„°ì…‹ì„ ì„ íƒí•˜ì„¸ìš” (2ê°œ ì´ìƒ ê¶Œì¥):",
        options=list(available_options.keys()),
        default=list(available_options.keys())[:2] if len(available_options) >= 2 else list(available_options.keys())
    )

    if len(selected_names) == 0:
        st.warning("âš ï¸ ìµœì†Œ 1ê°œ ì´ìƒì˜ ë°ì´í„°ì…‹ì„ ì„ íƒí•´ì£¼ì„¸ìš”.")
        return

    # Load selected datasets
    datasets_to_overlay = []
    dataset_colors = ['red', 'blue', 'green', 'purple', 'orange', 'darkred', 'darkblue']

    for idx, name in enumerate(selected_names):
        dataset_key = available_options[name]
        df = load_dataset_from_session(dataset_key)

        if df is not None:
            lat_col, lng_col = detect_lat_lng_columns(df)

            if lat_col and lng_col:
                popup_candidates = [col for col in df.columns if col not in [lat_col, lng_col]]
                popup_cols = popup_candidates[:3]

                datasets_to_overlay.append({
                    'df': df,
                    'lat_col': lat_col,
                    'lng_col': lng_col,
                    'popup_cols': popup_cols,
                    'color': dataset_colors[idx % len(dataset_colors)],
                    'name': name,
                    'icon': 'info-sign'
                })
            else:
                st.warning(f"âš ï¸ {name} ë°ì´í„°ì…‹ì—ì„œ ì¢Œí‘œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # Display overlay map (T024)
    if datasets_to_overlay:
        st.subheader("ğŸ—ºï¸ í†µí•© ì§€ë„ ì‹œê°í™”")

        # Show legend
        st.markdown("**ë²”ë¡€:**")
        legend_cols = st.columns(min(len(datasets_to_overlay), 4))
        for idx, ds in enumerate(datasets_to_overlay):
            with legend_cols[idx % 4]:
                st.markdown(f"ğŸ”µ **{ds['name']}** ({len(ds['df']):,}ê°œ)")

        # Create and display map
        overlay_map = create_overlay_map(datasets_to_overlay)
        st_folium(overlay_map, width=900, height=600)

        st.info("ğŸ’¡ ì§€ë„ ìš°ì¸¡ ìƒë‹¨ì˜ ë ˆì´ì–´ ì»¨íŠ¸ë¡¤ì„ ì‚¬ìš©í•˜ì—¬ ê° ë°ì´í„°ì…‹ì„ ê°œë³„ì ìœ¼ë¡œ ì¼œê³  ëŒ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    else:
        st.warning("âš ï¸ ì¢Œí‘œ ì •ë³´ê°€ ìˆëŠ” ë°ì´í„°ì…‹ì´ ì—†ìŠµë‹ˆë‹¤.")


def render_sidebar():
    """
    Render the sidebar with API key input and status. (T041-T044)
    """
    with st.sidebar:
        st.header("ğŸ¤– AI ì„¤ì •")

        # T041: API Key input
        api_key = st.text_input(
            "Anthropic API Key",
            type="password",
            placeholder="sk-ant-...",
            help="Anthropic API Keyë¥¼ ì…ë ¥í•˜ì„¸ìš”. https://console.anthropic.com ì—ì„œ ë°œê¸‰ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
            key="sidebar_api_key"
        )

        if api_key:
            if validate_api_key(api_key):
                st.session_state.chatbot['api_key'] = api_key
                st.success("âœ… API Key ì„¤ì •ë¨")
            else:
                st.error("âŒ API Key í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤")
        else:
            st.info("API Keyë¥¼ ì…ë ¥í•˜ë©´ AI ì±—ë´‡ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤")

        # T042: Model selection
        st.subheader("ëª¨ë¸ ì„ íƒ")
        model_options = {opt['name']: opt['id'] for opt in AI_MODEL_OPTIONS}
        model_descriptions = {opt['name']: opt['description'] for opt in AI_MODEL_OPTIONS}

        selected_model_name = st.selectbox(
            "AI ëª¨ë¸",
            options=list(model_options.keys()),
            help=model_descriptions.get(list(model_options.keys())[0], ""),
            key="sidebar_model"
        )
        st.session_state.chatbot['model'] = model_options[selected_model_name]
        st.caption(model_descriptions[selected_model_name])

        # T043: Token usage display
        st.subheader("ğŸ“Š í† í° ì‚¬ìš©ëŸ‰")
        tokens = st.session_state.chatbot['tokens']
        col1, col2 = st.columns(2)
        with col1:
            st.metric("ì…ë ¥", f"{tokens['input']:,}")
        with col2:
            st.metric("ì¶œë ¥", f"{tokens['output']:,}")
        st.metric("ì´ê³„", f"{tokens['total']:,}")

        st.markdown("---")

        # T044: Upload status display
        st.subheader("ğŸ“ ë°ì´í„° ì—…ë¡œë“œ í˜„í™©")
        uploaded_count = sum(st.session_state.upload_status.values())
        st.progress(uploaded_count / len(DATASET_MAPPING))
        st.caption(f"{uploaded_count} / {len(DATASET_MAPPING)} ë°ì´í„°ì…‹ ì—…ë¡œë“œë¨")

        for key, status in st.session_state.upload_status.items():
            icon = "âœ…" if status else "â³"
            st.text(f"{icon} {DATASET_MAPPING[key]['display_name']}")


def render_chatbot_tab():
    """
    Render the chatbot tab for data Q&A. (T045-T050)
    """
    st.header("ğŸ’¬ ë°ì´í„° ì§ˆì˜ì‘ë‹µ")
    st.markdown("ì—…ë¡œë“œí•œ ë°ì´í„°ì…‹ì— ëŒ€í•´ AIì—ê²Œ ì§ˆë¬¸í•˜ì„¸ìš”.")

    # T050: Check API Key
    api_key = st.session_state.chatbot.get('api_key', '')
    if not api_key or not validate_api_key(api_key):
        st.warning("âš ï¸ ì‚¬ì´ë“œë°”ì—ì„œ Anthropic API Keyë¥¼ ë¨¼ì € ì…ë ¥í•´ì£¼ì„¸ìš”.")
        st.info("""
        **API Key ë°œê¸‰ ë°©ë²•:**
        1. [Anthropic Console](https://console.anthropic.com) ë°©ë¬¸
        2. ê³„ì • ìƒì„± ë˜ëŠ” ë¡œê·¸ì¸
        3. API Keys ë©”ë‰´ì—ì„œ ìƒˆ í‚¤ ìƒì„±
        4. ìƒì„±ëœ í‚¤ë¥¼ ì‚¬ì´ë“œë°”ì— ì…ë ¥
        """)
        return

    # Check uploaded datasets
    uploaded_datasets = {
        DATASET_MAPPING[key]['display_name']: key
        for key, uploaded in st.session_state.upload_status.items()
        if uploaded
    }

    if not uploaded_datasets:
        st.info("ğŸ“¤ ë¨¼ì € **í”„ë¡œì íŠ¸ ê°œìš”** íƒ­ì—ì„œ ë°ì´í„°ì…‹ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
        return

    # T046: Dataset selection
    selected_display_name = st.selectbox(
        "ë¶„ì„í•  ë°ì´í„°ì…‹ ì„ íƒ:",
        options=list(uploaded_datasets.keys()),
        key="chatbot_dataset"
    )
    selected_dataset_key = uploaded_datasets[selected_display_name]
    st.session_state.chatbot['selected_dataset'] = selected_dataset_key

    # Load selected dataset
    df = load_dataset_from_session(selected_dataset_key)
    if df is None:
        st.error("ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return

    # Show dataset summary
    with st.expander("ğŸ“Š ë°ì´í„°ì…‹ ìš”ì•½", expanded=False):
        col1, col2, col3 = st.columns(3)
        with col1:
            st.metric("í–‰ ìˆ˜", f"{len(df):,}")
        with col2:
            st.metric("ì»¬ëŸ¼ ìˆ˜", len(df.columns))
        with col3:
            missing_pct = df.isnull().sum().sum() / (len(df) * len(df.columns)) * 100
            st.metric("ì „ì²´ ê²°ì¸¡ë¥ ", f"{missing_pct:.1f}%")
        st.dataframe(df.head(3), use_container_width=True)

    st.markdown("---")

    # T049: Display conversation history
    st.subheader("ëŒ€í™” ë‚´ì—­")

    for msg in st.session_state.chatbot['messages']:
        with st.chat_message(msg['role']):
            st.markdown(msg['content'])

    # T047, T048: Question input and send
    user_question = st.chat_input("ë°ì´í„°ì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”...")

    if user_question:
        # Add user message to history
        st.session_state.chatbot['messages'].append({
            'role': 'user',
            'content': user_question
        })

        # Display user message
        with st.chat_message('user'):
            st.markdown(user_question)

        # Generate response
        with st.chat_message('assistant'):
            with st.spinner("AIê°€ ë‹µë³€ì„ ìƒì„±í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
                try:
                    # Create Anthropic client
                    client = Anthropic(api_key=api_key)

                    # Create data context
                    data_context = create_data_context(df, selected_display_name)

                    # Prepare messages for API
                    api_messages = [
                        {'role': m['role'], 'content': m['content']}
                        for m in st.session_state.chatbot['messages']
                    ]

                    # Get response
                    response_text, usage = create_chat_response(
                        client=client,
                        model=st.session_state.chatbot['model'],
                        messages=api_messages,
                        data_context=data_context
                    )

                    # Update token usage
                    st.session_state.chatbot['tokens']['input'] += usage['input_tokens']
                    st.session_state.chatbot['tokens']['output'] += usage['output_tokens']
                    st.session_state.chatbot['tokens']['total'] += (
                        usage['input_tokens'] + usage['output_tokens']
                    )

                    # Display response
                    st.markdown(response_text)

                    # Add assistant message to history
                    st.session_state.chatbot['messages'].append({
                        'role': 'assistant',
                        'content': response_text
                    })

                except Exception as e:
                    error_msg = handle_chat_error(e)
                    st.error(error_msg)

    # Clear conversation button
    if st.session_state.chatbot['messages']:
        if st.button("ğŸ—‘ï¸ ëŒ€í™” ë‚´ì—­ ì‚­ì œ", key="clear_chat"):
            st.session_state.chatbot['messages'] = []
            st.rerun()


def main():
    """Main application entry point."""
    # Initialize session state
    init_session_state()

    # Render sidebar (T041-T044)
    render_sidebar()

    st.title("ğŸ“Š ëŒ€êµ¬ ê³µê³µë°ì´í„° ì‹œê°í™”")
    st.markdown("""
    ëŒ€êµ¬ ê³µê³µë°ì´í„° ì‹œê°í™” ë„êµ¬ì— ì˜¤ì‹  ê²ƒì„ í™˜ì˜í•©ë‹ˆë‹¤! ì´ ì• í”Œë¦¬ì¼€ì´ì…˜ì€
    7ê°œì˜ ê³µê³µ ë°ì´í„°ì…‹ì„ íƒìƒ‰í•˜ê³  ëŒ€í™”í˜• ì‹œê°í™”ë¥¼ í†µí•´ ê³µê°„ íŒ¨í„´ì„ ë°œê²¬í•  ìˆ˜ ìˆë„ë¡ ë„ì™€ì¤ë‹ˆë‹¤.
    """)

    # Create tabs - T013: í”„ë¡œì íŠ¸ ê°œìš” first, T014: tab names, T015: ë°ì´í„° ì§ˆì˜ì‘ë‹µ ì¶”ê°€
    tabs = st.tabs([
        "ğŸ“– í”„ë¡œì íŠ¸ ê°œìš”",
        "ğŸ¥ CCTV",
        "ğŸ’¡ ë³´ì•ˆë“±",
        "ğŸ« ì–´ë¦°ì´ ë³´í˜¸êµ¬ì—­",
        "ğŸ…¿ï¸ ì£¼ì°¨ì¥",
        "ğŸš— ì‚¬ê³ ",
        "ğŸ“Š í›ˆë ¨ ë°ì´í„°",
        "ğŸ“‹ í…ŒìŠ¤íŠ¸ ë°ì´í„°",
        "ğŸ”„ êµì°¨ ë°ì´í„° ë¶„ì„",
        "ğŸ’¬ ë°ì´í„° ì§ˆì˜ì‘ë‹µ"
    ])

    # Tab 0: Project Overview (with upload)
    with tabs[0]:
        render_overview_tab()

    # Tab 1: CCTV
    with tabs[1]:
        render_dataset_tab('cctv', 'CCTV')

    # Tab 2: Security Lights
    with tabs[2]:
        render_dataset_tab('lights', 'ë³´ì•ˆë“±')

    # Tab 3: Child Protection Zones
    with tabs[3]:
        render_dataset_tab('zones', 'ì–´ë¦°ì´ ë³´í˜¸êµ¬ì—­')

    # Tab 4: Parking Lots
    with tabs[4]:
        render_dataset_tab('parking', 'ì£¼ì°¨ì¥')

    # Tab 5: Accident
    with tabs[5]:
        render_dataset_tab('accident', 'ì‚¬ê³ ')

    # Tab 6: Train Data (T014: renamed)
    with tabs[6]:
        render_dataset_tab('train', 'í›ˆë ¨ ë°ì´í„°')

    # Tab 7: Test Data (T014: renamed)
    with tabs[7]:
        render_dataset_tab('test', 'í…ŒìŠ¤íŠ¸ ë°ì´í„°')

    # Tab 8: Cross-Data Analysis (T023, T024: simplified)
    with tabs[8]:
        render_cross_analysis_tab()

    # Tab 9: Chatbot (T045-T050)
    with tabs[9]:
        render_chatbot_tab()


if __name__ == "__main__":
    main()
